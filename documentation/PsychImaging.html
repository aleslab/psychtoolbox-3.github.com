<h2 id="psychtoolbox-psychglimageprocessing">[[Psychtoolbox]] › [[PsychGLImageProcessing]]</h2>
<p>rc = <a href="PsychImaging" class="uri">PsychImaging</a>(subcommand [,arg1][,arg2][….,argn]) - Control common<br />
functions of the Psychtoolbox GPU image processing pipeline.</p>
<p>This function allows you to setup and control various aspects and common<br />
functions of the Psychtoolbox image processing pipeline in a simple way.<br />
Various standard scenarious can be conveniently set up with this routine,<br />
e.g., geometric transformations of your stimulus image, various types of<br />
display correction, …</p>
<p>If you want to perform less common, unusual or simply not yet supported tasks<br />
with the pipeline, use the low-level <a href="Screen" class="uri">Screen</a>(‘HookFunction’, …)<br />
interface instead and have a peek in the M-File code for the<br />
<a href="PsychImaging" class="uri">PsychImaging</a>.m file to learn about the low-level interface.<br />
See “help <a href="PsychGLImageprocessing" class="uri">PsychGLImageprocessing</a>” for more info.</p>
<h3 id="subcommands-and-their-meaning">Subcommands and their meaning:</h3>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘PrepareConfiguration’);<br />
- Prepare setup of imaging pipeline for onscreen window.<br />
This is the first step in the sequence of configuration steps.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichChannel, whichTask [,param1]…);<br />
- Add a specific task or processing requirement to the list of actions<br />
to be performed by the pipeline for the currently selected onscreen<br />
window. ‘whichChannel’ is a string with the name of the channel to<br />
configure:</p>
<p>‘LeftView’ applies the action to the processing channel<br />
for the left-eye view of a stereo configuration. ‘RightView’ applies the<br />
action to the right-eye view of a stereo configuration. ‘AllViews’ applies<br />
the action to both, left- and right eye view channels of a stereo<br />
configuration or to the single monoscopic channel of a mono display<br />
configuration. Other options are ‘Compositor’, ‘FinalFormatting’ and<br />
‘Finalizer’ for special purpose channels. Set this to ‘General’ if the<br />
command doesn’t apply to a specific view, but is a general requirement.</p>
<p>‘whichTask’ contains the name string of one of the supported<br />
actions:</p>
<p>* ‘UseGPGPUCompute’ Enable use of <a href="GeneralPurposeGPU" class="uri">GeneralPurposeGPU</a> computing support.<br />
This prepares use of Psychtoolbox functions which are meant to<br />
interface with, or take advantage of, the general purpose computation<br />
capabilities of modern graphics processing units and other massively<br />
parallel compute acceleration hardware, e.g., DSP’s, or multi-core<br />
processors. Interfacing with such hardware is done via common standard<br />
compute API’s like NVidia’s CUDA or the cross-platform <a href="OpenCL" class="uri">OpenCL</a> API.</p>
<p>Use of this function often requires specific modern GPU hardware and<br />
the installation of additional driver software, e.g., NVidia’s freely<br />
available CUDA SDK and runtime, or the free and open-source <a href="GPUmat" class="uri">GPUmat</a><br />
toolbox. Read ‘help PsychGPGPU’ for further info.</p>
<p>This function just detects and selects supported GPU compute API’s for<br />
use with Psychtoolbox and initializes them and some Psychtoolbox<br />
function to take advantage if appropriate. While you could use those<br />
API’s by themselves without calling this init function, Psychtoolbox<br />
builtin processing functions would not be able to take advantage of the<br />
API’s or perform efficient and fast data exchange with them.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseGPGPUCompute’, apitype [, flags]);</p>
<p>‘apitype’ Allows selection of the compute API to use. The value ‘Auto’<br />
leaves the choice to Psychtoolbox. The value ‘GPUmat’ selects the<br />
high-level, free and open-source <a href="GPUmat" class="uri">GPUmat</a> compute toolkit for Matlab.<br />
Currently no other choices are supported, but this is expected to<br />
change in the future.</p>
<p>‘flags’ An optional string of keyword flags to determine behaviour.<br />
There aren’t any flags defined yet.</p>
<p>* ‘SideBySideCompressedStereo’ <a href="Ask" class="uri">Ask</a> for stereo display in a horizontally<br />
compressed side-by-side format. Left and Right eye images are drawn at<br />
full framebuffer resolution by usercode. <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’, …) draws them<br />
horizontally compressed side-by-side to each other. They are scanned<br />
out to the display device this way and then the display device itself<br />
uncompresses them back to full resolution and displays them<br />
stereoscopically, typically via built-in alternating frame-sequential<br />
stereo with stereo goggles, but other methods are conceivable. This is<br />
one popular stereo frame packing format for stereo on HDMI display<br />
devices. Once you’ve set up a stereo display mode via <a href="PsychImaging" class="uri">PsychImaging</a>, you<br />
can tweak its specific parameters by calling the function<br />
<a href="SetCompressedStereoSideBySideParameters" class="uri">SetCompressedStereoSideBySideParameters</a>().</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘SideBySideCompressedStereo’);</p>
<p>* ‘InterleavedColumnStereo’ <a href="Ask" class="uri">Ask</a> for stereo display in interleaved mode.<br />
The output image is composed from the lefteye and righteye stereo<br />
buffers by interleaving their content: Even columns are filled with<br />
content from the left buffer, odd columns are filled with content from<br />
the right buffer, i.e., Col 0 = Left col 0, Col 1 = Right Col 0, Col 2<br />
= Left col 1, Col 3 = Right col 1, ….</p>
<p>This mode is useful for driving some auto-stereoscopic displays. These<br />
use either some vertical parallax barriers or vertical lenticular<br />
lense sheets. These direct light from even columns to one eye, light<br />
from odd columns to the other eye.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘InterleavedColumnStereo’, startright);</p>
<p>If ‘startright’ is zero, then even columns are taken from left buffer. If<br />
‘startright’ is one, then even columns are taken from the right buffer.</p>
<p>You can use the <a href="RemapMouse" class="uri">RemapMouse</a>() function to correct <a href="GetMouse" class="uri">GetMouse</a>() positions<br />
for potential geometric distortions introduced by this function.</p>
<p>* ‘InterleavedLineStereo’ <a href="Ask" class="uri">Ask</a> for stereo display in interleaved mode.<br />
The output image is composed from the lefteye and righteye stereo<br />
buffers by interleaving their content: Even lines are filled with<br />
content from the left buffer, odd lines are filled with content from<br />
the right buffer, i.e., Row 0 = Left row 0, Row 1 = Right row 0, Row 2<br />
= Left row 1, Row 3 = Right row 1, ….</p>
<p>This mode is useful for driving some types of stereo devices and<br />
goggles, e.g., the iGlasses 3D Video goggles in interleaved stereo<br />
mode.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘InterleavedLineStereo’, startright);</p>
<p>If ‘startright’ is zero, then even lines are taken from left buffer. If<br />
‘startright’ is one, then even lines are taken from the right buffer.</p>
<p>You can use the <a href="RemapMouse" class="uri">RemapMouse</a>() function to correct <a href="GetMouse" class="uri">GetMouse</a>() positions<br />
for potential geometric distortions introduced by this function.%</p>
<p>* ‘DualWindowStereo’ <a href="Ask" class="uri">Ask</a> for stereo display in dual-window mode (stereomode 10)</p>
<p>Only use this function under <a href="MacOSX" class="uri">MacOSX</a>. If possible on your setup and OS,<br />
rather use a single window, spanning both stereo display outputs, and use<br />
stereomode 4 or 5 to display dual-display stereo. That is much more<br />
efficient in terms of speed, computational load and memory consumption,<br />
also potentially more robust with respect to visual stimulation timing.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘DualWindowStereo’, rightEyeScreen [, rightEyeWindowRect]);</p>
<p>The left-eye image will be displayed on the screen and at a location<br />
specified as usual via <a href="PsychImaging" class="uri">PsychImaging</a>(‘Openwindow’, screenid, …, rect);<br />
The right eye image will be displayed on screen ‘rightEyeScreen’. If<br />
the optional ‘rightEyeWindowRect’ is specified, then the right eye image<br />
is not displayed in a fullscreen window, but in a window with the bounding<br />
rectangle ‘rightEyeWindowRect’.</p>
<p>* ‘UseVirtualFramebuffer’ <a href="Ask" class="uri">Ask</a> for support of virtual framebuffer, even if<br />
it isn’t strictly needed, given the remaining set of requirements. Most<br />
of the tasks require such a framebuffer - it gets enabled anyway. In a<br />
few cases, e.g., to simplify your code (no need for special cases), it<br />
may be useful to activate such a framebuffer even if it isn’t strictly<br />
needed. This option activates a minimal buffer with 8 bits per color<br />
cmponent fixed point precision.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseVirtualFramebuffer’);</p>
<p>* ‘UseSubpixelDrive’ <a href="Ask" class="uri">Ask</a> to take advantage of the so-called “Subpixel Drive”<br />
mode of certain monochromatic medical imaging displays like, e.g., the<br />
“Eizo <a href="RadiForce" class="uri">RadiForce</a> GS-521”. This monitor essentially has a RGB panel with<br />
horizontal RGB subpixels, but with the color filters removed, so each<br />
pixel is horizontally split up into 3 luminance subpixels and these can<br />
be individually addressed by packing 3 horizontally adjacent 8 bit stimulus<br />
luminance pixels into the “RGB” color channels of an output pixel. Use<br />
of this task will create a virtual framebuffer three times the width of<br />
the output framebuffer and then pack triplets of three horizontal luminance<br />
pixels into one output pixel, to triple the effective horizontal resolution.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseSubpixelDrive’);</p>
<p>* ‘UseRetinaResolution’ <a href="Ask" class="uri">Ask</a> to prefer a framebuffer with the full native<br />
resolution of attached <a href="HiDPI" class="uri">HiDPI</a> “Retina” displays on OSX, instead of a scaled<br />
down lower resolution framebuffer with typically half the horizontal<br />
and vertical resolution of the Retina display. This setting will be<br />
ignored if the onscreen window is not displayed on a <a href="HiDPI" class="uri">HiDPI</a> “Retina”<br />
display in a scaled display mode, or if the panel fitter is in use by<br />
specifying the ‘UsePanelFitter’ task. By default, <a href="Screen" class="uri">Screen</a>() would use a<br />
downscaled framebuffer on a Retina display under OSX and scale that low<br />
resolution buffer up to full display panel resolution, just as Apples<br />
OSX operating system does it by default. This in order to reduce<br />
computational load, improve graphics performance, and avoid problems<br />
with backward compatibility of old code. If you want to make full use<br />
of the resolution of your <a href="HiDPI" class="uri">HiDPI</a> display, specify this task to tell<br />
<a href="Screen" class="uri">Screen</a>() to use the full display panel resolution on OSX, even if this may<br />
introduce some compatibility issues into your code and causes a decrease<br />
in graphics performance due to the higher graphics rendering load.</p>
<p>If ‘UseRetinaResolution’ is used with a non-fullscreen window, ie.<br />
the ‘rect’ parameter in <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’, …) is provided<br />
to specify the screen position and size of the window, note that<br />
the size of the window rect returned by <a href="Screen" class="uri">Screen</a>(‘GlobalRect’) and<br />
<a href="Screen" class="uri">Screen</a>(‘Rect’), as well as of the returned rect of <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’)<br />
will differ from the size of the ‘rect’ passed to ‘OpenWindow’. ‘rect’s<br />
passed into <a href="OpenWindow" class="uri">OpenWindow</a> for positioning and sizing the window, as well<br />
as the global position rect returned by <a href="Screen" class="uri">Screen</a>(’GlobalRect’) for the<br />
current size and position of the onscreen window are expressed in global<br />
desktop coordinates, in somewhat arbitrary units of virtual “points”.<br />
How such a point translates into display pixels depends on the operating<br />
system, possibly the desktop GUI in use (on other systems than OSX), the<br />
set of connected displays and their Retina or non-Retina resolutions.<br />
The aim is that the coordinate system is somewhat consistent and meaningful<br />
across all connected displays, for varying definitions of “consistent” and<br />
“meaningful” on different operating systems, but the mapping of points to<br />
physical screen pixels can be different on each connected display, at the<br />
discretion of the operating system. You may get especially “interesting”<br />
results if you try to move an onscreen window between screens, or let it<br />
span multiple displays of different type and resolution.<br />
The rect returned by <a href="PsychImaging" class="uri">PsychImaging</a>(‘Openwindow’) and <a href="Screen" class="uri">Screen</a>(‘Rect’), as<br />
well as sizes returned by <a href="Screen" class="uri">Screen</a>(‘WindowSize’) define the net useable<br />
size of the window in display pixels. It is affected by all kind of<br />
<a href="PsychImaging" class="uri">PsychImaging</a> operations, e.g., selection of stereo modes, high bit depth<br />
modes etc., but also by scaling on Retina displays in high res mode.<br />
If ‘UseRetinaResolution’ is used on a Retina/<a href="HiDPI" class="uri">HiDPI</a> display, one typical<br />
result will be that the size of the window in pixels reported by these<br />
functions will be higher than the size in points, as one virtual point will<br />
get represented by more than 1 pixel on a Retina display. Observing twice<br />
the window size in pixels than in points is quite typical, but other<br />
scaling factors are possible. The take home message for you is to specify<br />
location and size of your stimuli based on the sizes and rects returned<br />
by <a href="Screen" class="uri">Screen</a>(‘Rect’), <a href="PsychImaging" class="uri">PsychImaging</a>(‘Openwindow’) and <a href="Screen" class="uri">Screen</a>(‘Windowsize’), as<br />
these are in units of display pixels, and *not* based on the virtual points<br />
returned by <a href="Screen" class="uri">Screen</a>(‘GlobalRect’). The 2nd take home message is that you<br />
should mostly use fullscreen windows for visual stimulation to avoid such<br />
and other pitfalls.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseRetinaResolution’);</p>
<p>* ‘UseDisplayRotation’ <a href="Ask" class="uri">Ask</a> to use builtin panel fitter exclusively for<br />
rotating the framebuffer. This is useful if you want to turn your<br />
display device from landscape (= normal upright) orientation into<br />
portrait orientation (= rotated by 90 degrees clockwise or<br />
counterclockwise). In such a case you will want to rotate the<br />
framebuffer by 90 degrees as well, but you should *not* use the “rotate<br />
monitor” function of your operating system for this purpose, as this<br />
will very likely interfere with visual stimulus presentation timing and<br />
timestamping! Use this task instead. It will perform rotation in a<br />
similar way, but without severe interference to timing. However, there<br />
is one limitation to this method: Multisample anti-aliasing currently<br />
does not work if you use our framebuffer rotation.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseDisplayRotation’, angle);</p>
<p>‘angle’ is the desired rotation angle. The only values which will give<br />
well defined and useful results are multiples of 90 degrees, useful<br />
values are essentially 0, +90, -90 and 180 degrees for no rotation,<br />
clockwise rotation, counterclockwise rotation and upside down rotation.</p>
<p>This function is mutually exclusive with ‘UsePanelFitter’, but if you<br />
need to use both, you can omit ‘UseDisplayRotation’ and pass the<br />
‘angle’ parameter to ‘UsePanelFitter’ instead, which also accepts an<br />
‘angle’ parameter with the same meaning.</p>
<p>This function is not very mature yet: If you want to use the<br />
panelfitter for anything beyond simple framebuffer rotation by 90<br />
degree increments, you will likely hit bugs or limitations which will<br />
require significant tinkering by you.</p>
<p>* ‘UsePanelFitter’ <a href="Ask" class="uri">Ask</a> to use builtin panel fitter. This allows you to<br />
define a virtual size for your onscreen window. The window will behave<br />
as if it had that virtual size wrt. all size queries and drawing<br />
operations. However, at <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’) time, the visual content of the<br />
window will be resized by a fast scaling operation to the real size of<br />
the windows framebuffer, ie., its real onscreen size. Scaling uses<br />
bilinear interpolation or better for high quality results. After<br />
rescaling to the real size, post-processing and display of your<br />
stimulus image will proceed at full resolution. This function is useful<br />
if you want to display a stimulus designed for a specific display<br />
resolution on a display device of different higher or lower resolution.<br />
Given that size and shape of the virtual framebuffer and real display<br />
window may not match, the function provides you with multiple possible<br />
choices on how to rescale your stimulus image, e.g., to maximize<br />
display area, or to preserve the aspect ratio of the original image,<br />
trading off displayed area etc.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UsePanelFitter’, size, strategy [, srcRect, dstRect][, angle]);</p>
<p>‘size’ is a [width, height] vector defining the width x height of the<br />
virtual window in pixels.</p>
<h3 id="strategy-a-text-string-selecting-the-scaling-method.-following-settings-are-possible">‘strategy’ a text string selecting the scaling method. Following settings are possible:</h3>
<p>‘Full’ - <a href="Scale" class="uri">Scale</a> to full window size. Aspect ratio is not preserved,<br />
unless the virtual window and the real onscreen windows ‘rect’<br />
already have the same aspect ratio, in which case this will be<br />
a simple scaling operation.</p>
<p>‘Aspect’ - <a href="Scale" class="uri">Scale</a> to maximum size possible while preserving aspect<br />
ratio. This will center the stimulus and add black<br />
horizontal or vertical borders as neccessary.</p>
<p>‘AspectWidth’ - <a href="Scale" class="uri">Scale</a> aspect ratio preserving to cover full display<br />
width. Cut off top and bottom content if neccessary.</p>
<p>‘AspectHeight’ - <a href="Scale" class="uri">Scale</a> aspect ratio preserving to cover full display<br />
height. Cut off left and right content if neccessary.</p>
<p>‘Centered’ - Center stimulus without any scaling, add black borders<br />
around stimulus or cut away border regions to get a<br />
one-to-one mapping.</p>
<p>‘Custom’ - This works like the ‘srcRect’ and ‘dstRect’ parameters of<br />
<a href="Screen" class="uri">Screen</a>(‘DrawTexture’): Cut out a ‘srcRect’ region from the<br />
virtual framebuffer and display it in the ‘dstRect’ region.<br />
‘srcRect’ and ‘dstRect’ are given in typical [left, top, right, bottom]<br />
format.</p>
<p>‘angle’ is an optional rotation angle. If provided and non-zero, the<br />
panelfitter will also rotate the output framebuffer by the given<br />
rotation angle. Note: This doesn’t work very well yet with most<br />
framebuffer sizes and scaling strategies. What does work is if the<br />
specified ‘size’ is identical to the onscreen windows size, or is its<br />
transposed size (ie., if window is width x height pixels, then height x<br />
width pixels will work as ‘size’ parameter) and the rotation angle is a<br />
multiple of 90 degrees. This is mostly useful for display rotation from<br />
landscape orientation into portrait orientation. Your mileage with<br />
other configurations or rotation angles will vary.</p>
<p>Example: Suppose your real window covers a 1920 x 1080 display.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UsePanelFitter’, [800 600], ‘Aspect’);<br />
-&gt; This would give you a virtual window of 800 x 600 pixels to draw<br />
into and would rescale the 800 x 600 stimulus image to 1440 x 1080<br />
pixels and display it centered on the 1920 x 1080 pixels display.<br />
Aspect ratio would be correct and the image would cover the full height<br />
1080 pixels of the display, but only 1440 out of 1920 pixels of its<br />
width, thereby leaving black borders on the left and right side of your<br />
stimulus.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UsePanelFitter’, [800 600], ‘AspectHeight’);<br />
-&gt; Would do the same as above.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UsePanelFitter’, [800 600], ‘AspectWidth’);<br />
-&gt; Would create a final image of 1920 pixels width, as you asked to<br />
cover the full display width, aspect ratio would be correct, but the<br />
top and bottom 75 pixels of your original stimulus would get cut away,<br />
because they wouldn’t fit after scaling without distorting the image.</p>
<p>* ‘UseFastOffscreenWindows’ <a href="Ask" class="uri">Ask</a> for support of fast Offscreen windows.<br />
These use a more efficient storage, backed by <a href="OpenGL" class="uri">OpenGL</a> framebuffer<br />
objects (FBO’s). Drawing into them isn’t faster, but *switching*<br />
between drawing into onscreen- and offscreen windows, or switching<br />
between drawing into different offscreen windows is faster. They also<br />
support a couple of other advanced features and performance<br />
improvements in conjunction with the imaging pipeline.<br />
If you only specify this task, then you’ll get the benefit of fast<br />
windows, without the cost of other features of the pipeline you might<br />
not need.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseFastOffscreenWindows’);</p>
<p>* ‘EnableCLUTMapping’ Enable support for old-fashioned clut animation /<br />
clut mapping. The drawn framebuffer image is transformed by applying a<br />
color lookup table (clut). This is not done via the hardware gamma<br />
tables as in the good ol’ days, but by application of the clut via<br />
image processing. Hardware gamma tables don’t provide well defined<br />
timing on modern hardware, therefore they aren’t suitable anymore.</p>
<p>You can update the clut to be applied at the next <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’);<br />
via the command <a href="Screen" class="uri">Screen</a>(‘LoadNormalizedGammatable’, windowPtr, clut, 2);</p>
<p>‘clut’ needs to be a clutSize-by-3 matrix, with ‘clutSize’ slots and<br />
one column for each of the red, green and blue color channels.</p>
<h3 id="setup-command">Setup command:</h3>
<p>By default, a clut of 256 slots with (R,G,B) values is used, but you<br />
can provide the optional ‘clutSize’ parameter to use clut’s with more<br />
slots. The maximum number depends on your GPU, but 2048 are typically<br />
supported even on very low-end cards.</p>
<p>If you set ‘highprecision’ to 1, the clut will resolve values at more<br />
than 8 bit per color channel on modern hardware. This usually only<br />
makes sense if you also use a more than 8 bpc framebuffer with more<br />
than 256 slots as clutSize.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichView, ‘EnableCLUTMapping’ [, clutSize=256][, highprecision=0]);<br />
Example: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘AllViews’, ‘EnableCLUTMapping’);</p>
<p>* ‘FloatingPoint16Bit’ <a href="Ask" class="uri">Ask</a> for a 16 bit floating point precision<br />
framebuffer. This allows more than 8 bit precision for complex drawing,<br />
compositing and image processing operations. It also allows<br />
alpha-blending with signed color values and intermediate results that<br />
are outside the displayable range, e.g., negative. Precision is about 3<br />
digits behind the decimal point or 1024 discriminable displayable<br />
levels. If you need higher precision, choose ‘FloatingPoint32Bit’.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘FloatingPoint16Bit’);</p>
<p>* ‘FixedPoint16Bit’ <a href="Ask" class="uri">Ask</a> for a 16 bit integer precision framebuffer.<br />
On graphics hardware that supports this, a 16 bit signed integer<br />
framebuffer will be created. Such a framebuffer can store intermediate<br />
color values in the normalized range [-1.0 ; +1.0] with a precision of<br />
15 bits per component. Only positive values between 0.0 and 1.0 are<br />
displayable in the end though. If the graphics hardware does not support this,<br />
a 16 bit unsigned integer framebuffer is tried instead. Such a framebuffer<br />
allows for 16 bits of precision per color component. However, many graphics<br />
cards do not support alpha-blending on such a framebuffer, and<br />
intermediate out-of-range values (smaller than zero or bigger than one) aren’t<br />
supported either. Such values will be clamped to the representable [0.0 ; 1.0]<br />
range instead. Additionally this mode is only supported on some graphics<br />
hardware. It is a special purpose intermediate solution - more accurate<br />
than 16 bit floating point, but less capable and less accurate than 32<br />
bit floating point. If you need higher precision, choose ‘FloatingPoint32Bit’.</p>
<p>The main sad reason this switch exists is because some graphics hardware or<br />
graphics drivers do not support floating point precision textures and<br />
framebuffers due to some ridiculous patent restrictions, but they do<br />
support a 16 bit signed or unsigned integer precision format. The switch<br />
is basically a workaround for the broken patent systems of many countries.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘FixedPoint16Bit’);</p>
<p>* ‘FloatingPoint32Bit’ <a href="Ask" class="uri">Ask</a> for a 32 bit floating point precision<br />
framebuffer. This allows more than 8 bit precision for complex drawing,<br />
compositing and image processing operations. It also allows<br />
alpha-blending with signed color values and intermediate results that<br />
are outside the displayable range, e.g., negative. Precision is about<br />
6.5 digits behind the dezimal point or 8 million discriminable displayable<br />
levels. Be aware that only the most recent hardware <a href="(NVidia)%20Geforce%208000%20series,%20ATI%20Radeon%20HD%202000%20series">(NVidia</a> is able to perform<br />
alpha-blending at full speed in this mode. Enabling alpha-blending on<br />
older hardware may cause a significant decrease in drawing performance,<br />
or alpha blending may not work at all at this precision! If you’d like<br />
to have both, the highest precision and support for alpha-blending,<br />
specify ‘FloatingPoint32BitIfPossible’ instead. PTB will then try to<br />
use 32 bit precision if this is possible in combination with alpha<br />
blending. Otherwise, it will choose 16 bit precision for drawing &amp;<br />
blending, but 32 bit precision at least for the post-processing.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘FloatingPoint32Bit’);</p>
<p>* ‘FloatingPoint32BitIfPossible’ <a href="Ask" class="uri">Ask</a> PTB to choose the highest precision<br />
that is possible on your hardware without sacrificing functionality like,<br />
e.g., alpha-blending. PTB will choose the best compromise possible for<br />
your hardware setup.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘FloatingPoint32BitIfPossible’);</p>
<p>* ‘NormalizedHighresColorRange’ <a href="Ask" class="uri">Ask</a> PTB to use a normalized range of<br />
color and luminance intensity levels in the interval [0; 1], ie. values<br />
between zero and one for minimum and maximum intensity. Also ask for<br />
unclamped colors – intermediate results are allowed to take on<br />
arbitrary values, e.g., also negative values. All <a href="Screen" class="uri">Screen</a>() 2D drawing<br />
commands should operate at maximum color/luminance precision.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘NormalizedHighresColorRange’ [, applyAlsoToMakeTexture]);</p>
<p>The command <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘NormalizedHighresColorRange’, 1);<br />
is automatically executed if you used <a href="PsychDefaultSetup" class="uri">PsychDefaultSetup</a>(featureLevel)<br />
with a featureLevel of &gt;= 2 at the top of your experiment script,<br />
*except* that clamping is *not* disabled by default in this case! To<br />
disable clamping you’d still need to add this task explicitely, as<br />
unclamping may have unintended side effects on old graphics hardware.</p>
<p>The optional flag ‘applyAlsoToMakeTexture’ defaults to zero. If set to 1,<br />
then a unit color range of expected input values in the [0; 1] range is<br />
also applied to standard 8-Bit precision textures in <a href="Screen" class="uri">Screen</a>(‘MakeTexture’)<br />
if the provided Matlab imageMatrix is of double precision type instead of<br />
uint8 type. This allows to specify standard textures in the same consistent<br />
value range 0-1 as other drawing colors, for cleaner code. Such textures<br />
will still be limited to 0-1 range and only resolved into 256 intensity<br />
levels, unless you also set the optional ‘floatprecision’ flag in <a href="Screen" class="uri">Screen</a>(‘MakeTexture’)<br />
to a value of 1 or 2. We still apply this limitation, as high precision textures consume<br />
more memory and other resources and are incompatible with very old graphics<br />
hardware.</p>
<p>This is just a convenience shortcut for <a href="Screen" class="uri">Screen</a>(‘ColorRange’, win, 1, 0, applyAlsoToMakeTexture);<br />
with the added benefit of allowing to specify the background clear<br />
color in normalized 0-1 range as well. This command is implied by use<br />
of any of the high precision display device drivers (for attenuators,<br />
Bits+ box etc.). It is only needed if you want to create the same<br />
visual results on a 8 bit standard framebuffer without needing to<br />
change your code, or if you want to set the ‘applyAlsoToMakeTexture’ flag to a<br />
setting of non-zero, so unit colorrange also applies to <a href="Screen" class="uri">Screen</a>(‘MakeTexture’).</p>
<p>* ‘StereoCrosstalkReduction’ If a stereoMode is active or requested,<br />
apply a shader first in the processing chain that for each eye aims to<br />
reduce crosstalk from the other eye.</p>
<h3 id="usage">Usage:</h3>
<pre><code>[PsychImaging](PsychImaging)(&#39;AddTask&#39;, &#39;LeftView&#39;, &#39;StereoCrosstalkReduction&#39;, method, crossTalkGain);  
[PsychImaging](PsychImaging)(&#39;AddTask&#39;, &#39;RightView&#39;, &#39;StereoCrosstalkReduction&#39;, method, crossTalkGain);  </code></pre>
<p>The ‘method’ parameter selects the method to use for crosstalk<br />
reduction.</p>
<h3 id="currently-only-a-method-named-subtractother-is-implemented-which-works-as-follows">Currently only a method named ‘SubtractOther’ is implemented, which works as follows:</h3>
<p>To reduce crosstalk, the contrast in the image of each eye, i.e., the<br />
difference in color from the background level provided as background<br />
clear color of the window is subtracted from the image of the other eye,<br />
after scaling the contrast by ‘crossTalkGain’. ‘crossTalkGain’ can be a<br />
scalar, or a separate gain for each RGB channel. The background color<br />
can be a scalar in the range 0-1, or a 3-element array to set the<br />
backgroundlevel for each RGB channel separately. The background<br />
color level should not be zero, as contrast then can’t be inverted<br />
around the background level. In general, the background level<br />
should be high enough to allow unclamped inversion of the highest<br />
contrast features of your stimulus at your ‘crossTalkGain’, or<br />
artifacts will occur.</p>
<p>* ‘DisplayColorCorrection’ Select a method for color correction to apply to<br />
stimuli before output conversion and display. You have to specify a<br />
color correction method ‘methodname’ to apply as parameter, see “help<br />
<a href="PsychColorCorrection" class="uri">PsychColorCorrection</a>” for an overview of supported color correction<br />
methods and their adjustable parameters. The imaging pipeline will be<br />
set up to support the chosen color correction method. After you’ve<br />
opened the onscreen window, you can use the different subcommands of<br />
<a href="PsychColorCorrection" class="uri">PsychColorCorrection</a>() to change parameters of the color correction<br />
algorithm at runtime.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichView, ‘DisplayColorCorrection’, methodname);</p>
<p>Example: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘FinalFormatting’, ‘DisplayColorCorrection’, ‘SimpleGamma’);<br />
This would apply a simple power-law gamma correction to all view<br />
channels of a stereo setup, or the single view of a monoscopic setup.<br />
Later on you could use the methods of <a href="PsychColorCorrection" class="uri">PsychColorCorrection</a>() to<br />
actually set the wanted gamma correction factors.</p>
<p>Please note that we use the channel ‘FinalFormatting’ instead of<br />
‘AllViews’ as we’d usually do. Both specs will work, but a selection<br />
of ‘FinalFormatting’ will lead to faster processing in many cases, so<br />
this is preferred here if you want to apply the same setting to all<br />
view channels - or to a single monoscopic display. Should you find<br />
that things don’t work as expected, you might try ‘AllViews’ instead<br />
of ‘FinalFormatting’ - There are subtle differences in how they<br />
process your instructions, which may matter in some corner cases.</p>
<p>* ‘EnablePseudoGrayOutput’ Enable the high-performance driver for the<br />
rendering of up to 1786 different levels of gray on a standard - but<br />
well calibrated - color monitor and 8 bit graphics card. This is done<br />
by applying an algorithn known as “Pseudo-Gray” or “Bit stealing”.<br />
Selecting this mode implies use of 32 bit floating point<br />
framebuffers, unless you specify use of a 16 bit floating point<br />
framebuffer via ‘FloatingPoint16Bit’ explicitely. The expected range<br />
of input luminance values is between 0 and 1. See “help <a href="CreatePseudoGrayLUT" class="uri">CreatePseudoGrayLUT</a>”<br />
for further explanation.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnablePseudoGrayOutput’);</p>
<p>* ‘EnableGenericHighPrecisionLuminanceOutput’<br />
Setup Psychtoolbox for conversion of high precision luminance images<br />
into a format suitable for special high precision luminance display<br />
devices. This is a generic support routine that uses LUT based<br />
conversion.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableGenericHighPrecisionLuminanceOutput’, lut);</p>
<p>* ‘EnableVideoSwitcherSimpleLuminanceOutput’<br />
Setup Psychtoolbox for conversion of high precision luminance images<br />
into a format suitable for driving the “<a href="VideoSwitcher" class="uri">VideoSwitcher</a>” high precision<br />
luminance display device which was developed by Xiangrui Li et al.</p>
<p>This implements the simple converter, which only needs the<br />
Blue-To-Red-Ratio of the device as input parameter and performs<br />
conversion via a closed-form formula without any need for lookup<br />
tables. This is supposed to be fast.</p>
<p>See “help <a href="VideoSwitcher" class="uri">VideoSwitcher</a>” for more info about the device and its<br />
options.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableVideoSwitcherSimpleLuminanceOutput’ [, btrr] [, trigger]);</p>
<ul>
<li><p>The optional ‘btrr’ parameter is the Blue-To-Red-Ratio to use. If the<br />
parameter is left out, the btrr value will be read from a global<br />
configuration file.</p></li>
<li><p>The optional ‘trigger’ parameter can be zero for “No trigger”, or 1<br />
for “Use trigger as configured”. By default, trigger is off (==0).<br />
Enabled, one can use the <a href="VideoSwitcher" class="uri">VideoSwitcher</a>(‘SetTrigger’, …); function to<br />
configure when and how a trigger signal should be emitted. Trigger<br />
signals are simply specific pixel patterns in the green output channel.<br />
That channel is recognized by the <a href="VideoSwitcher" class="uri">VideoSwitcher</a> as a trigger signal<br />
control channel.</p></li>
</ul>
<p>* ‘EnableVideoSwitcherCalibratedLuminanceOutput’<br />
Setup Psychtoolbox for conversion of high precision luminance images<br />
into a format suitable for driving the “<a href="VideoSwitcher" class="uri">VideoSwitcher</a>” high precision<br />
luminance display device which was developed by Xiangrui Li et al.</p>
<p>This implements the simple converter, which only needs the<br />
Blue-To-Red-Ratio of the device as input parameter and performs<br />
conversion via a closed-form formula without any need for lookup<br />
tables. This is supposed to be fast.</p>
<p>See “help <a href="VideoSwitcher" class="uri">VideoSwitcher</a>” for more info about the device and its<br />
options.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableVideoSwitcherCalibratedLuminanceOutput’ [, btrr] [, lut] [, trigger]);</p>
<ul>
<li><p>The optional ‘btrr’ parameter is the Blue-To-Red-Ratio to use. If the<br />
parameter is left out, the btrr value will be read from a global<br />
configuration file.</p></li>
<li><p>The optional ‘lut’ paramter is a 257 elements vector of luminance<br />
values, which maps blue channel drive indices to luminance values. This<br />
lut needs to be acquired via a calibration procedure by use of a<br />
photometer. If ‘lut’ is left out, the table will be read from a global<br />
configuration file.</p></li>
<li><p>The optional ‘trigger’ parameter can be zero for “No trigger”, or 1<br />
for “Use trigger as configured”. By default, trigger is off (==0).<br />
Enabled, one can use the <a href="VideoSwitcher" class="uri">VideoSwitcher</a>(‘SetTrigger’, …); function to<br />
configure when and how a trigger signal should be emitted. Trigger<br />
signals are simply specific pixel patterns in the green output channel.<br />
That channel is recognized by the <a href="VideoSwitcher" class="uri">VideoSwitcher</a> as a trigger signal<br />
control channel.</p></li>
</ul>
<p>* ‘EnableNative10BitFramebuffer’ Enable support for output of stimuli<br />
with 10 bit precision per color channel (10 bpc / 30 bpp / “Deep color”)<br />
on graphics hardware that supports native 10 bpc framebuffers.</p>
<p>Many graphics cards of the professional class AMD/ATI Fire series<br />
(2008 models and later) and all current models of the professional class<br />
<a href="NVidia" class="uri">NVidia</a> Quadro series (2008 models and later), as well as all current models<br />
of the consumer class <a href="NVidia" class="uri">NVidia</a> <a href="GeForce" class="uri">GeForce</a> series under Linux, do support 10 bpc<br />
framebuffers under some circumstances. 10 bpc display on classic CRT monitors<br />
which are connected via analog VGA outputs is supported. Support for digital<br />
display devices like LCD/OLED panels or video projectors depends on the specific<br />
type of display output connector used, the specific panels, and their video<br />
settings. Consult manufacturer documentation for details. In general, 10 bpc<br />
output may be supported on some graphics cards and displays via <a href="DisplayPort" class="uri">DisplayPort</a><br />
or HDMI video outputs, but to our knowledge not via DVI-D outputs.</p>
<p>If such a combination of graphics card and display is present on your system<br />
on Linux or Microsoft Windows, then Psychtoolbox will request native support<br />
from the standard graphics drivers, ie., it won’t need to use our own<br />
homegrown, experimental box of tricks to enable this. You do need to enable/<br />
unlock 10 bpc mode somewhere in the display driver settings though. On Linux you<br />
can do this for supported cards and drivers via <a href="XOrgConfCreator" class="uri">XOrgConfCreator</a> + <a href="XOrgConfSelector" class="uri">XOrgConfSelector</a>,<br />
on Windows the method is vendor specific.</p>
<p>Apple OSX, since version 10.11.2 “El Capitan”, does support native 10 bpc video<br />
output on some small subset of Apple hardware, as of May 2017 these are the <a href="MacPro" class="uri">MacPro</a><br />
2013 “with some suitable displays” (Apple quotation), and the 27 inch iMac models<br />
late 2014 and late 2015 with Retina 5k displays. We’ve confirmed this to be working<br />
on the iMac 5k Retina 27 inch late 2014 model via photometer measurements. On OSX,<br />
the OS will actually provide a 16 bit half-float framebuffer for our onscreen windows.<br />
This buffer provides ~11 bpc effective linear precision in the displayable color<br />
intensity range 0.0-1.0. The OS outputs this 11 bpc framebuffer as a native 10 bpc<br />
video signal on suitable displays and uses some Apple proprietary software spatial<br />
dithering algorithm to add 1 extra bit of simulated precision, so a photometer would<br />
measure up to 11 bpc perceived/measured precision. On some other Mac models, which are<br />
not in Apples list of 10 bit capable Macs, Apple uses a proprietary spatial dithering<br />
algorithm implemented in software to fake a precision of 11 bpc on standard 8 bpc<br />
framebuffers and displays, at least convincing enough for a photometer. The downside<br />
of this proprietary dithering scheme is that visual stimulus onset timing precision<br />
and timestamping precision is impaired, so Macs in “10 bit” framebuffer mode are not<br />
suitable if trustworthy frame accurate visual timing is needed. Nothing we could do<br />
about this. To summarize: <a href="EnableNative10BitFramebuffer" class="uri">EnableNative10BitFramebuffer</a> mode on OSX will actually give<br />
you a simulated 11 bpc framebuffer on some Mac hardware, plus severe visual timing<br />
problems.</p>
<h3 id="psychtoolbox-experimental-10-bpc-framebuffer-support">Psychtoolbox experimental 10 bpc framebuffer support:</h3>
<p>Additionally we support ATI/AMD Radeon hardware of the X1000, HD2000 - HD8000,<br />
series and later models (everything since at least the year 2006) under Linux<br />
via our own low-level setup mechanisms. These graphics cards support a native<br />
ARGB2101010 framebuffer, ie., a system framebuffer with 2 bits for the alpha<br />
channel, and 10 bits per color channel.</p>
<p>As this is supported by the hardware, but not always by the standard AMD<br />
graphics drivers, we follow a hybrid approach: We use special low-level<br />
hardware access to reconfigure the hardware for 10 bpc framebuffer support.<br />
Then we use a special imaging pipeline formatting plugin to convert 16 bpc or<br />
32 bpc stimuli into the special data format required by this framebuffer<br />
configuration.</p>
<p>On Linux you must have run <a href="PsychLinuxConfiguration" class="uri">PsychLinuxConfiguration</a> at least once on your<br />
system at some point. You’ll need to have one of the supported AMD Radeon<br />
gfx-cards (see above) for this to work. If you use Linux with the free and<br />
open-source AMD graphics drivers, 10 bpc framebuffer support should work<br />
reliably, so use of the open-source drivers on Linux is recommended for<br />
reliable results.</p>
<p>Getting a 10 bpc framebuffer working is only the first half of what you need for<br />
high color precision output. Your graphics card must also be able to transmit the<br />
video signal at high precision to the display device and the display must be able<br />
to faithfully reproduce the high precision image. 10 bpc output has been verified<br />
to work for analog VGA connected CRT monitors and displays on both AMD and <a href="NVidia" class="uri">NVidia</a><br />
graphics cards which do support 10 bpc framebuffers, so with a analog VGA CRT you<br />
should be safe. Note that this only applies to native VGA output via VGA connectors<br />
or passive DVI-I to VGA adapters connected to a DVI-I connector. Active <a href="DisplayPort" class="uri">DisplayPort</a><br />
to VGA adapters or active HDMI to VGA adapters may be limited to maximum 8 bpc output.<br />
The status of 10 bpc output to digital display devices differs a lot across devices<br />
and OS’es. Output of 10 bpc framebuffers to standard 8 bpc digital laptop panels or</p>
<p>DVI/HDMI/<a href="DisplayPort" class="uri">DisplayPort</a> panels via digital dithering is known to work, but that is not<br />
the real thing, only a simulation of 10 bpc via dithering to 8 bpc. This may or may<br />
not be good enough for your specific visual stimulation paradigm. On a DVI-D connected<br />
standard digital display, this dithered output is the best you will ever get.</p>
<p><a href="DisplayPort" class="uri">DisplayPort</a>: Recent <a href="NVidia" class="uri">NVidia</a> and AMD graphics cards can output to some suitable <a href="DisplayPort" class="uri">DisplayPort</a><br />
displays with 10 bpc or higher precision on Linux, and maybe also on MS-Windows, but you<br />
have to verify this carefully for your specific display.</p>
<p>HDMI: Recent Intel graphics cards can output up to 12 bpc precision to HDMI deep color<br />
capable displays on Linux, and maybe also on MS-Windows. However, &gt; 8 bpc framebuffers<br />
are not yet supported, so this can only be used for gamma correction. All AMD graphics<br />
cards of model Radeon HD-5000 or later (and equivalent Fire-Series models) can output to<br />
HDMI deep color capable displays with 10 bpc real precision at least if you use a Linux<br />
kernel of version 3.16 or later with the open-source AMD graphics drivers. Execute<br />
<a href="PsychLinuxConfiguration" class="uri">PsychLinuxConfiguration</a> to enable this &gt;= 10 bpc deep color output mode, then reboot your<br />
machine once to enable it.</p>
<p>The status with the proprietary AMD drivers on Linux or on MS-Windows is unknown.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableNative10BitFramebuffer’ [, disableDithering=0]);</p>
<p>This function will setup a 32 bpc floating point resolution framebuffer by<br />
default for Psychtoolbox drawing and stimulus processing. Output will happen<br />
into a 10 bpc framebuffer. The function will also disable the graphics cards<br />
gamma tables, so you’ll need to use <a href="PsychImaging" class="uri">PsychImaging</a>(…‘DisplayColorCorrection’…)<br />
for gamma and color correction if you need this.</p>
<p>The function will *not* disable dithering on digital displays by default,<br />
but leave that decision to the operating system and graphics drivers of<br />
your machine. A well working OS would disable dithering on a 10 bpc or<br />
higher color depth display, if the display reports its capability to the<br />
OS via its EDID info. It would enable dithering on &lt; 10 bpc displays, so<br />
you’d get a “pseudo 10 bpc” display where 10 bpc color depths is simulated<br />
on a 6 bpc or 8 bpc display via the dithering.</p>
<p>You can disable dithering manually on some graphics cards by providing the<br />
optional ‘disableDithering’ flag as 1. Currently mostly AMD cards allow this<br />
control. <a href="NVidia" class="uri">NVidia</a> or Intel cards require manual setup to force dithering off.</p>
<p>* ‘EnableNative11BitFramebuffer’ Enable support for output of stimuli with (almost)<br />
11 bit precision per color channel (11 bpc / 32 bpp / “Deep color”) on graphics<br />
hardware that supports native 11 bpc framebuffers. This will request an ~ 11 bpc<br />
framebuffer from the operating system. If it can’t get such a framebuffer on Linux<br />
with AMD graphics hardware, it will use our own homegrown setup code to provide<br />
such a framebuffer anyway on Radeon X1000, HD-2000 and later graphics cards and<br />
equivalent Fire-Series graphics cards. On OSX 10.11.2 it will request and get a<br />
~11 bpc framebuffer on some Mac models. See the explanations above for 10 bpc on<br />
OSX.</p>
<p>Read all the explanations in the section above for ‘EnableNative10BitFramebuffer’<br />
for capabilities, limitations and possible caveats on different systems.</p>
<p>Please note that on Linux this “11 Bit framebuffer” is not quite 11 bpc precision,<br />
but only about ~ 10.6666 bpc precision. Specifically, the framebuffer can only<br />
store at most 32 bits of color information per pixel, so it will store 11 bit<br />
precision for the red channel (2048 distinct red intensity levels), 11 bit<br />
(2048 levels) for the green channel, but only 10 bit (1024 levels) for the blue<br />
channel, for a total number of 11 + 11 + 10 bits = 32 bits of color information<br />
per pixel, or 4 billion different possible colors. A true 11 bpc framebuffer would<br />
need 33 bits per pixel, and current graphics hardware can’t handle that.</p>
<p>How many bits of precision of these ~ 11 bpc actually reach your display device?</p>
<ul>
<li><p>Analog VGA only provides for maximum 10 bpc output precision on all shipping<br />
<a href="NVidia" class="uri">NVidia</a> and AMD graphics cards at best. Intel graphics cards only allow for 8 bpc.</p></li>
<li><p><a href="DisplayPort" class="uri">DisplayPort</a> or HDMI might allow for transfer of 11 bpc precision, in general they<br />
support up to 12 bpc. However additional hardware restrictions for your graphics<br />
card may limit precision to as low as 10 bpc. To our knowledge, only AMD graphics<br />
cards support ~ 11 bpc framebuffers at all. Radeon HD-7000 and earlier can only<br />
truly process up to 10 bpc, so ‘EnableNative11BitFramebuffer’ may not gain you any<br />
precision over ‘EnableNative10BitFramebuffer’ in practice on these cards. AMD cards<br />
of the “Sea Islands” family or later, mostly models from the year &gt;= 2014, should be<br />
able to process and output up to 12 bpc over HDMI or <a href="DisplayPort" class="uri">DisplayPort</a>, so they’d be able<br />
to output true ~11 bpc images. However, this hasn’t been verified by us so far due to<br />
lack of suitable hardware - we don’t know if it really works.</p></li>
</ul>
<p>So obviously: Measure very carefully on your setup what kind of precision you really<br />
get and make sure not to be fooled by dithering if you need precise low-level control<br />
of spatial stimulus properties, or per-pixel high precision, instead of just averaged<br />
over larger clusters of pixels.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableNative11BitFramebuffer’ [, disableDithering=0]);</p>
<p>* ‘EnableNative16BitFramebuffer’ Enable up to 16 bpc, 64 bpp framebuffer on some setups.<br />
This asks to enable a framebuffer with a color depth of up to 16 bpc for up to 65535 levels<br />
of intensity per red, green and blue channel or 48 bits = different 2^48 colors. Currently,<br />
as of June 2017, this mode of operation is only supported on Linux when using the open-source<br />
FOSS radeon graphics drivers on modern AMD graphics cards, and only after some special config-<br />
uration of your X-Server and display setup has been performed by you. This is essentially a<br />
low-level hack that works under those specific conditions, but uses a relatively large amount<br />
of graphics memory and compute resources to implement. If you can do with less than 12 bpc, you<br />
are better off with the other high bit depth modes, as they are easier to set up and more efficient<br />
and faster in operation. On suitable setups, this will establish a 16 bpc framebuffer which packs<br />
3 * 16 bpc = 48 bit color info into 64 bpp pixels and the gpu’s display engine will scan out that<br />
framebuffer at 16 bpc. However, effective output precision is further limited to &lt; 16 bpc by your<br />
display, video connection and specific model of graphics card. As of June 2017, the maximum effective<br />
output precision is limited to at most 12 bpc (4096 levels of red, green and blue) by the graphics<br />
card, and this precision may only be attainable on AMD graphics cards of the so called “Sea Islands”<br />
(cik) family when used with the radeon-kms display driver. Any older or more recent cards, e.g.,<br />
“Southern Islands” or “Volcanic Islands” will not work with this hack. The specific requirement is<br />
an AMD gpu with a “DCE-8 or later” display engine that uses the old/classic ati/radeon-ddx and<br />
radeon-kms display driver, not the new amdgpu-ddx / amdgpu-kms driver. Cards older than “Sea Islands”<br />
don’t have a DCE-8+ engine, and cards newer than “Sea Islands” don’t work with the classic radeon<br />
driver anymore, so effectively only “Sea Islands” (cik) DCE-8.x gpu’s may work with this hack.<br />
Please note that actual 12 bpc output precision even in the best case scenario has not been verified<br />
by us so far, due to lack of suitable 12 bpc display hardware, so this mode is highly experimental and<br />
may not work at all.</p>
<p>High bit depth output would only work over HDMI or <a href="DisplayPort" class="uri">DisplayPort</a>, and may be further restricted by<br />
your specific display device, so measure your results carefully! See the sections about 11 bpc and<br />
10 bpc native framebuffers above for further details.</p>
<h3 id="required-manual-one-time-setup">Required manual one time setup:</h3>
<ol type="1">
<li><p>You must create a custom made xorg.conf file for your graphics card and X-Server to setup<br />
the display screen for use of a linear, non-tiled framebuffer at a color depth of 24 bit.<br />
Setup for this is easily achieved via <a href="XOrgConfCreator" class="uri">XOrgConfCreator</a> on supported gpu’s and drivers:</p>
<ol type="a">
<li><p>Use <a href="XOrgConfCreator" class="uri">XOrgConfCreator</a> to create a custom xorg.conf file for this purpose. If it asks for<br />
“Special setup options” answer ’y’es. When it asks for use of a linear, non-tiled framebuffer,<br />
answer ’y’es. This will create a proper config file. Use <a href="XOrgConfSelector" class="uri">XOrgConfSelector</a> to select that file.</p></li>
<li><p>Logout and login again, so the display server picks up the changed configuration.</p></li>
</ol>
<p>If you need a more customized xorg.conf file for special settings or for more complex display and<br />
gpu setups, use our template file as a reference. The important bit is to add the “<a href="ColorTiling" class="uri">ColorTiling</a>…”<br />
lines to the “Device” section for your AMD graphics card.</p></li>
<li><p>Only three distinct display setups are allowed: Either a single display connected, or if multiple<br />
displays are conected, all displays must mirror (aka clone) each other showing the same image,<br />
or a dual display setup with both displays running at the same video resolution, one display<br />
showing the left half of your onscreen window, the other showing the right half of your onscreen<br />
window, ie., a typical setup for dual-display side-by-side stereo presentation. Pretty much any other<br />
display setup will display undefined results, e.g., corrupted images or random pixel trash.<br />
Also note that not all desktop GUI environments work: GNOME-3 desktop “Gnome shell” is known to work<br />
at least as tested on Ubuntu Linux 16.04 LTS. Ubuntu Unity may work. KDE-5 usually does not work,<br />
other desktop GUI’s are not tested.</p></li>
</ol>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableNative16BitFramebuffer’ [, disableDithering=0][, bpc]);</p>
<p>* ‘EnableBrightSideHDROutput’ Enable the high-performance driver for<br />
<a href="BrightSide" class="uri">BrightSide</a> Technologies High dynamic range display device for 16 bit<br />
per color channel output precision. See “help <a href="BrightSideHDR" class="uri">BrightSideHDR</a>” for<br />
detailed explanation. Please note that you’ll need to install the 3rd<br />
party driver libraries for that display as described in the help file.<br />
PTB doesn’t come bundled with that libraries for copyright reasons.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableBrightSideHDROutput’);</p>
<p>* ‘UseDataPixx’ Tell Psychtoolbox that additional functionality for<br />
displaying the onscreen window on a <a href="VPixx" class="uri">VPixx</a> Technologies <a href="DataPixx" class="uri">DataPixx</a> device<br />
should be enabled.</p>
<p>This command is implied by enabling a <a href="DataPixx" class="uri">DataPixx</a> video mode by one of the<br />
commands for the <a href="DataPixx" class="uri">DataPixx</a> in the following sections.</p>
<p>‘UseDataPixx’ mostly prepares use of a variety of subfunctions in the<br />
<a href="DataPixxToolbox" class="uri">DataPixxToolbox</a> (“help <a href="DataPixxToolbox" class="uri">DataPixxToolbox</a>”) and in the <a href="PsychDataPixx" class="uri">PsychDataPixx</a>()<br />
high-level driver (“help <a href="PsychDataPixx" class="uri">PsychDataPixx</a>”).</p>
<p>* ‘EnableDataPixxL48Output’ Setup Psychtoolbox for L48 mode of the <a href="VPixx" class="uri">VPixx</a><br />
Technologies <a href="DataPixx" class="uri">DataPixx</a> device. This loads the graphics hardwares gamma<br />
table with an identity mapping so it can’t interfere with <a href="DPixx" class="uri">DPixx</a> video<br />
processing. It also sets up automatic generation of control signals to<br />
support the features of <a href="DPixx" class="uri">DPixx</a> that are available via the functions in<br />
<a href="PsychDataPixx" class="uri">PsychDataPixx</a>(). You will be able to upload new CLUT’s into the <a href="DPixx" class="uri">DPixx</a><br />
by use of the <a href="Screen" class="uri">Screen</a>(‘LoadNormalizedGammaTable’, window, clut, 2);<br />
command. CLUT updates will be synchronized with <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’) commands.<br />
Please note that while L48 CLUT mode works even with very old<br />
graphics hardware, this is a pretty cumbersome way of driving the<br />
<a href="DPixx" class="uri">DPixx</a>. On recent hardware, you will want to use M16 or C48 mode<br />
(see below). That allows to draw arbitrarily complex stimuli with as<br />
many colors as you want and PTB will take care of conversion into the<br />
M16 or C48 format for <a href="DataPixx" class="uri">DataPixx</a>.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableDataPixxL48Output’);</p>
<p>* ‘EnableDataPixxM16Output’ Enable the high-performance driver for M16<br />
mode of the <a href="VPixx" class="uri">VPixx</a> Technologies <a href="DataPixx" class="uri">DataPixx</a> device. This is the fastest and<br />
most elegant way of driving the <a href="DPixx" class="uri">DPixx</a> box with 16 bit luminance output<br />
precision. See “help <a href="DataPixx" class="uri">DataPixx</a>” for more information. Selecting this<br />
mode implies use of 32 bit floating point framebuffers, unless you<br />
specify use of a 16 bit floating point framebuffer via<br />
‘FloatingPoint16Bit’ explicitely. If you do that, you will not be able<br />
to use the full 16 bit output precision, but only approximately 10 bits.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableDataPixxM16Output’);</p>
<p>If you want to make use of the color overlay plane in M16 mode, then<br />
call the function like this:</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableDataPixxM16OutputWithOverlay’);<br />
See the explanation of color overlays in the section<br />
‘EnableBits++Mono++OutputWithOverlay’ - behaviour of color overlays is<br />
identical for the CRS Bits++ and the <a href="VPixx" class="uri">VPixx</a> <a href="DataPixx" class="uri">DataPixx</a>.</p>
<p>* ‘EnableDataPixxC48Output’ Enable the high-performance driver for the<br />
C48 mode of <a href="VPixx" class="uri">VPixx</a> technologies <a href="DataPixx" class="uri">DataPixx</a> box. This is the fastest and<br />
most elegant way of driving the <a href="DataPixx" class="uri">DataPixx</a> box with 16 bit per color<br />
channel output precision. See “help <a href="DataPixx" class="uri">DataPixx</a>” for more information.<br />
Selecting this mode implies use of 32 bit floating point framebuffers,<br />
unless you specify use of a 16 bit floating point framebuffer via<br />
‘FloatingPoint16Bit’ explicitely. If you do that, you will not be able<br />
to use the full 16 bit output precision, but only approximately 10 bits.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableDataPixxC48Output’, mode);</p>
<p>See the section below about ‘EnableBits++Color++Output’ for the meaning<br />
of the mandatory “mode” parameter.</p>
<p>You can use the <a href="RemapMouse" class="uri">RemapMouse</a>() function to correct <a href="GetMouse" class="uri">GetMouse</a>() positions<br />
for potential geometric distortions introduced by this function for<br />
“mode” zero.</p>
<p>* ‘UseBits#’ Tell Psychtoolbox that additional functionality for<br />
displaying the onscreen window on a Cambridge Research Systems Bits#<br />
device should be enabled.</p>
<p>This command is implied by enabling a Bits+ or Bits# video mode by one<br />
of the commands for the Bits+/Bits# in the following sections, if the<br />
driver can auto-detect a connected Bits# device. If it cannot auto-detect<br />
a connected Bits# device and this command is omitted, Psychtoolbox will<br />
instead assume that an older Bits+ is in use and only allow functionality<br />
common to Bits# and Bits+, without automatic video mode switching.</p>
<p>If you provide this command, you can optionally specify the name of the<br />
serial port to which your Bits# is connected, instead of leaving it to<br />
the system to find this out (either via configuration file or via a<br />
guess-o-matic).</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘UseBits#’ [, <a href="BitsSharpSerialPort" class="uri">BitsSharpSerialPort</a>]);</p>
<p>‘BitsSharpSerialPort’ is optional and can be set to the name of a serial<br />
port for your specific operating system and computer, to which the Bits#<br />
is connected. If omitted, Psychtoolbox will look for the name in the first<br />
line of text of a text file stored under the filesystem path and filename<br />
[<a href="PsychtoolboxConfigDir" class="uri">PsychtoolboxConfigDir</a> ‘BitsSharpConfig.txt’]. If that file is empty, the<br />
serial port is auto-detected (Good luck!).</p>
<p>‘UseBits#’ mostly prepares use of a variety of new Bits# subfunctions<br />
in the <a href="BitsPlusPlus" class="uri">BitsPlusPlus</a>() high-level driver (“help <a href="BitsPlusPlus" class="uri">BitsPlusPlus</a>”).</p>
<p>* ‘EnableBits++Bits++Output’ Setup Psychtoolbox for Bits++ mode of the<br />
Cambridge Research Systems Bits++ box. This loads the graphics<br />
hardwares gamma table with an identity mapping so it can’t interfere<br />
with Bits++ T-Lock system. It also sets up automatic generation of<br />
Bits++ T-Lock codes: You will be able to upload new CLUT’s into the<br />
Bits++ by use of the <a href="Screen" class="uri">Screen</a>(‘LoadNormalizedGammaTable’, window, clut, 2);<br />
command. CLUT updates will be synchronized with <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’)<br />
commands, because PTB will generate and draw the proper T-Lock code<br />
into the top line of your onscreen window. Please note that while<br />
Bits++ CLUT mode works even with very old graphics hardware, this is a<br />
pretty cumbersome way of driving the Bits++. On recent hardware, you<br />
will want to use Mono++ or Color++ mode (see below). That allows to<br />
draw arbitrarily complex stimuli with as many colors as you want and<br />
PTB will take care of conversion into the Color++ or Mono++ format for<br />
Bits++.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableBits++Bits++Output’);</p>
<p>* ‘EnableBits++Mono++Output’ Enable the high-performance driver for the<br />
Mono++ mode of Cambridge Research Systems Bits++ box. This is the<br />
fastest and most elegant way of driving the Bits++ box with 14 bit<br />
luminance output precision. See “help <a href="BitsPlusPlus" class="uri">BitsPlusPlus</a>” for more<br />
information. Selecting this mode implies use of 32 bit floating point<br />
framebuffers, unless you specify use of a 16 bit floating point<br />
framebuffer via ‘FloatingPoint16Bit’ explicitely. If you do that, you<br />
will not be able to use the full 14 bit output precision of Bits++, but<br />
only approximately 10 bits.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableBits++Mono++Output’);</p>
<p>If you want to make use of the color overlay plane in Mono++ mode, then<br />
call the function like this:</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableBits++Mono++OutputWithOverlay’);</p>
<h3 id="then-you-can-query-the-window-handle-of-the-overlay-window-via">Then you can query the window handle of the overlay window via:</h3>
<p>overlayWin = <a href="PsychImaging" class="uri">PsychImaging</a>(‘GetOverlayWindow’, window);</p>
<p>‘overlayWin’ is the handle to the overlay window associated with the<br />
overlay of onscreen window ‘window’. The overlay window is a standard<br />
offscreen window, so you can do anything with it that you would want to<br />
do with offscreen windows. The only difference is that the window is a<br />
pure index window: It only has one “color channel”, which can be written<br />
with color values between 0 and 255. Values 1 to 255 get mapped to the<br />
corresponding color indices of the Bits++ overlay plane: A zero value is<br />
transparent – Content of the onscreen window is visible. Positive<br />
non-zero color values map to the 255 indices available in overlay mode,<br />
these get mapped by the Bits++ CLUT to colors. You can define the<br />
mapping of indices to CLUT colors via the<br />
<a href="Screen" class="uri">Screen</a>(‘LoadNormalizedGammaTable’, window, clut, 2); command.</p>
<p>Updates of the overlay image are synchronized to <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’)<br />
updates. If you draw into the overlay window, the changed overlay image<br />
will become visible at <a href="Screen" class="uri">Screen</a>(‘<a href="Flip" class="uri">Flip</a>’) time – in sync with the changed<br />
onscreen window content. The overlay plane is not automatically cleared<br />
to background (or transparent) color after a flip, but its content<br />
persists across flips. You need to clear it out manually via a<br />
<a href="Screen" class="uri">Screen</a>(‘FillRect’) command.</p>
<p>* ‘EnableBits++Color++Output’ Enable the high-performance driver for the<br />
Color++ mode of Cambridge Research Systems Bits++ box. This is the<br />
fastest and most elegant way of driving the Bits++ box with 14 bit<br />
per color channel output precision. See “help <a href="BitsPlusPlus" class="uri">BitsPlusPlus</a>” for more<br />
information. Selecting this mode implies use of 32 bit floating point<br />
framebuffers, unless you specify use of a 16 bit floating point<br />
framebuffer via ‘FloatingPoint16Bit’ explicitely. If you do that, you<br />
will not be able to use the full 14 bit output precision of Bits++, but<br />
only approximately 10 bits.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableBits++Color++Output’, mode);</p>
<p>“mode” is a mandatory numeric parameter which must be 0, 1 or 2. In<br />
Color++ mode, the effective horizontal display resolution is only half<br />
the normal horizontal resolution. To cope with this, multiple different<br />
methods are implemented to squeeze your stimulus image horizontally by<br />
a factor of two. The following options exist:</p>
<p>0 = This is the “classic” mode which was used in all Psychtoolbox<br />
versions prior to 22nd September 2010. If you want to keep old code<br />
working as is, select 0. In this mode, your script will only see a<br />
framebuffer that is half the true horizontal resolution of your<br />
connected display screen. Each drawn pixel will be stretched to cover<br />
two pixels on the output display device horizontally. While this<br />
preserves the content of your stimulus image exactly, it means that the<br />
aspect ratio of all displayed text and stimuli will be 2:1. Text will<br />
be twice as wide as its height. Circles or squares will turn into<br />
horizontal ellipses or rectangles etc. You’ll need to do extra work in<br />
your code if you want to preserve aspect ratio properly.</p>
<p>You can use the <a href="RemapMouse" class="uri">RemapMouse</a>() function to correct <a href="GetMouse" class="uri">GetMouse</a>() positions<br />
for potential geometric distortions introduced by this function for<br />
“mode” zero.</p>
<p>Example: A fine vertical grid with alternating vertical white and black<br />
lines would display as expected, but each white or black stripe would be<br />
two pixels wide on the display instead of one pixel wide.</p>
<p>1 = Subsample: Your framebuffer will appear at the same resolution as<br />
your display device. Aspect ratio of drawn stimuli/text etc. will be<br />
correct and as expected. However, every 2nd column of pixels in your<br />
stimulus (ie., all odd-numbered x-coordinates 1,3,5,7,…) will be<br />
completely ignored, only even columns are used!</p>
<p>Example: A fine vertical grid with alternating vertical white and black<br />
lines would display as a purely white image, as only the white pixels<br />
in the even columns would be used, whereas the black pixels in the odd<br />
columns would be ignored.</p>
<p>2 = Average: Your framebuffer will appear at the same resolution as<br />
your display device. Aspect ratio of drawn stimuli/text etc. will be<br />
correct and as expected. However, each pair of adjacent even/odd pixel<br />
columns will be averaged before output. Stimulus pixels 0 and 1 will<br />
contribute the mean color for display pixel 0. Pixels 2 and 3 will be<br />
averaged into display pixel 1 and so on. Visually this gives the most<br />
pleasing and smooth results, but if adjacent even/odd pixels don’t have<br />
the same color value, you’ll obviously get an output color that is<br />
neither the color of the even pixel nor the odd pixel, but the average<br />
of both.</p>
<p>Example: A fine vertical grid with alternating vertical white and black<br />
lines would display as a 50% gray image, as the alternating white and<br />
black columns would be averaged into the average of white and black,<br />
which is 50% gray.</p>
<p>* ‘EnableDualPipeHDROutput’ Enable EXPERIMENTAL high-performance driver<br />
for HDR display devices which are composites of two separate displays.</p>
<p>EXPERIMENTAL proof-of-concept code with no real function yet!</p>
<p>This is meant for high-precision luminance or color output. It implies<br />
use of 32 bpc floating point framebuffers unless otherwise specified by<br />
other calls to <a href="PsychImaging" class="uri">PsychImaging</a>().</p>
<p>The pair of specially encoded output images that are derived from<br />
content of the onscreen window shall be output to both, the display<br />
associated with the screen given to <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’,…); and<br />
on the screen with the index ‘pipe1Screen’, using appropriate encoding<br />
to drive the HDR device or similar composite device.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘EnableDualPipeHDROutput’, pipe1Screen [, pipe1Rectangle]);</p>
<p>Optionally you can pass a ‘pipe1Rectangle’ if the window with the<br />
pipe1 image shall not fill the whole ‘pipe1Screen’, but only a<br />
subregion ‘pipe1Rectangle’.</p>
<p>* ‘AddOffsetToImage’ Add a constant color- or intensity offset to the<br />
drawn image, prior to all following image processing and post<br />
processing operations:<br />
Outimage(x,y) = Inimage(x,y) + Offset. If the framebuffer is in a color<br />
display mode, the same offset will be added to all three color<br />
channels.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichView, ‘AddOffsetToImage’, Offset);<br />
Example: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘AllViews’, ‘AddOffsetToImage’, 0.5);</p>
<p>* ‘MirrorDisplayTo2ndOutputHead’ Mirror the content of the onscreen<br />
window to given 2nd screen, ie., to a 2nd output connector (head)<br />
of a dualhead graphics card. This should give the same result as if one<br />
switches the graphics card into “Mirror mode” or “Clone mode” via the<br />
display settings panel of your operating system. Use of the “Mirror<br />
Mode” or “Clone Mode” of your operating system and graphics card is<br />
preferable to use of this command, if that works for you. The OS<br />
builtin facilities are usually faster, more efficient and thereby<br />
more reliable wrt. timing and synchronization!</p>
<p>This function only works for monoscopic displays, ie., it can not be<br />
used simultaneously with any stereo display mode. The reason is that it<br />
internally uses stereomode 10 with a few modifications to get its job<br />
done, so obviously neither mode 10 nor any other mode can be used<br />
without interference.</p>
<p>Only use this function for mirroring onto the 2nd head of a dual-head<br />
graphics card under <a href="MacOS" class="uri">MacOS</a>/X, or if you need to mirror onto a 2nd head<br />
on MS-Windows and can’t use “desktop spanning” mode on Windows to<br />
achieve dual display output. If possible on your setup and OS, rather use<br />
‘MirrorDisplayToSingleSplitWindow’ (see below). That mode should work<br />
well on dual-head graphics cards on MS-Windows or GNU/Linux, as well as<br />
in conjunction with a hardware display splitter attached to a single<br />
head on any operating system. It has the advantage of consuming less<br />
memory and compute ressources, so it is potentially faster or provides<br />
a more reliable overall timing.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘MirrorDisplayTo2ndOutputHead’, mirrorScreen [, mirrorRectangle]);</p>
<p>The content of the onscreen window shall be shown not only on the<br />
display associated with the screen given to <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’,<br />
…); but also (as a copy) on the screen with the index ‘mirrorScreen’.</p>
<p>Optionally you can pass a ‘mirrorRectangle’ if the window with the<br />
mirror image shall not fill the whole ‘mirrorScreen’, but only a<br />
subregion ‘mirrorRectangle’.</p>
<p>* ‘MirrorDisplayToSingleSplitWindow’ Mirror the content of the onscreen<br />
window to the right half of the desktop (if desktop spanning on a<br />
dual-display setup is enabled) or the right-half of the virtual screen<br />
if a display splitter (e.g., Matrox <a href="Dualhead2Go" class="uri">Dualhead2Go</a> (TM)) is attached to a<br />
single head of a graphics card. This should give the same result as if one<br />
switches the graphics card into “Mirror mode” or “Clone mode” via the<br />
display settings panel of your operating system. Use of the “Mirror<br />
Mode” or “Clone Mode” of your operating system and graphics card is<br />
preferable to use of this command, if that works for you. The OS<br />
builtin facilities are usually faster, more efficient and thereby<br />
more reliable wrt. timing and synchronization!</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘MirrorDisplayToSingleSplitWindow’);</p>
<p>Optionally, you can add the command…<br />
<a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, ‘DontUsePipelineIfPossible’);<br />
… if you don’t intend to use the imaging pipeline for anything else<br />
than display mirroring. This will allow further optimizations.</p>
<p>* ‘RestrictProcessing’ Restrict stimulus processing to a specific subarea<br />
of the screen. If your visual stimulus only covers a subarea of the<br />
display screen you can restrict PTB’s output processing to that<br />
subarea. This may save some computation time to allow for higher<br />
display redraw rates.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichChannel, ‘RestrictProcessing’, ROI);</p>
<p>ROI is a rectangle defining the area to process ROI = [left top right bottom];<br />
E.g., ROI = [400 400 800 800] would only create output pixels in the<br />
screen area with top-left corner (400,400) and bottom-right corner<br />
(800, 800).</p>
<p>* ‘FlipHorizontal’ and ‘FlipVertical’ flip your output images<br />
horizontally (left- and right interchanged) or vertically (upside down).</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichChannel, ‘FlipHorizontal’);<br />
Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichChannel, ‘FlipVertical’);</p>
<p>You can use the <a href="RemapMouse" class="uri">RemapMouse</a>() function to correct <a href="GetMouse" class="uri">GetMouse</a>() positions<br />
for potential geometric distortions introduced by this function.</p>
<p>* ‘GeometryCorrection’ Apply some geometric warping operation during<br />
rendering of the final stimulus image to correct for geometric<br />
distortion of your physical display device. You need to measure the<br />
geometric distortion of your display with a suitable calibration<br />
procedure, then compute an inverse warp transformation to undo this<br />
distortion, then provide that transformation to this function.</p>
<p>Usage: <a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, whichChannel, ‘GeometryCorrection’, calibfilename [, debugoutput] [, arg1], [arg2], …);</p>
<p>‘calibfilename’ is the filename of a calibration file which specified<br />
the type of undistortion to apply. Calibration files can be created by<br />
interactive calibration procedures. See ‘help CreateDisplayWarp’ for a<br />
list of calibration methods. One of the supported procedures is, e.g.,<br />
“<a href="DisplayUndistortionBezier" class="uri">DisplayUndistortionBezier</a>”, read “help <a href="DisplayUndistortionBezier" class="uri">DisplayUndistortionBezier</a>”. The<br />
recommended method for most cases is ‘DisplayUndistortionBVL’, read<br />
“help <a href="DisplayUndistortionBVL" class="uri">DisplayUndistortionBVL</a>” for help.</p>
<p>The optional flag ‘debugoutput’ if set to non-zero value will trigger<br />
some debug output about the calibration with some calibration methods.</p>
<p>The optional ‘arg1’, ‘arg2’, …, are optional parameters whose<br />
meaning depends on the calibration method in use.</p>
<p>Use of geometry correction will break the 1:1 correspondence between<br />
framebuffer pixel locations (x,y) and the mouse cursor position, ie. a<br />
mouse cursor positioned at display position (x,y) will be no longer<br />
pointing to framebuffer pixel (x,y). If you want to know which<br />
pixel in your original stimulus image corresponds to a specific<br />
physical display pixel (or mouse cursor position), use the function<br />
<a href="RemapMouse" class="uri">RemapMouse</a>() to perform the neccessary coordinate transformation.</p>
<p>* ‘UseVRHMD’ Display this onscreen window on a “Virtual Reality” head mounted<br />
display (HMD), e.g., the Oculus Rift DK1 or Rift DK2. This enables display of<br />
stereoscopic visual stimuli on supported virtual reality headsets.<br />
You need to have the neccessary vendor supplied VR runtimes installed for<br />
this to work.</p>
<h3 id="simple-usage">Simple usage:</h3>
<p>The most simple way to setup a HMD for use is to add a call to<br />
hmd = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘AutoSetupHMD’) instead of a call to<br />
<a href="PsychImaging" class="uri">PsychImaging</a>(‘AddTask’, ‘General’, UseVRHMD’, …). The ‘AutoSetupHMD’<br />
call would detect the first supported HMD device on your computer, connect to<br />
it, then set it up with reasonable default operating parameters. Then it would<br />
call this <a href="PsychImaging" class="uri">PsychImaging</a> task to perform all required setup steps.</p>
<h3 id="advanced-usage">Advanced usage:</h3>
<pre><code>1. Open a connection to a HMD and get a handle for the device:  
   For example, if you wanted to use a Oculus Rift DK1 or DK2, you could  
   do:  

   hmd = [PsychOculusVR](PsychOculusVR)(&#39;Open&#39; ...);  

2. Perform basic configuration of the HMD via the HMD specific driver.  

3. Add a [PsychImaging](PsychImaging) task for the HMD and pass in its device handle &#39;hmd&#39;:  
   [PsychImaging](PsychImaging)(&#39;AddTask&#39;, &#39;General&#39;, &#39;UseVRHMD&#39;, hmd);  </code></pre>
<p>This sequence will perform the necessary setup of panel fitter, stereo display<br />
mode and image post-processing for geometry correction, color aberration<br />
correction and vignette correction for a fullscreen window on the HMD.</p>
<p>* More actions will be supported in the future. If you can think of an<br />
action of common interest not yet supported by this framework, please<br />
file a feature request on our Wiki (Mainpage -&gt; Feature Requests).</p>
<p>After adding all wanted task specifications and other requirements,<br />
call…</p>
<p>[windowPtr, windowRect] = <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’, screenid, [backgroundcolor], ….);</p>
<ul>
<li>Finishes the setup phase for imaging pipeline, creates a suitable onscreen<br />
window and performs all remaining configuration steps. After this<br />
command, your onscreen window will be ready for drawing and display of<br />
stimuli. All specified imaging operations will get automatically applied<br />
to your stimulus before stimulus onset.</li>
</ul>
<p>After the window has been opened you can call the following commands any<br />
time at runtime:</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘RestrictProcessingToROI’, window, whichChannel, ROI);<br />
- Restrict the processing area of viewChannel ‘whichChannel’ of onscreen<br />
window ‘window’ to the rectangular subarea defined by ‘ROI’. See the<br />
explanation above for subtask ‘RestrictProcessing’. This does exactly the<br />
same but allows a dynamic change of the restricted area at any point<br />
during your experiment script.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘UnrestrictProcessing’, window, whichChannel);<br />
- Remove a restriction of the processing area of viewChannel<br />
‘whichChannel’ of onscreen window ‘window’ to a previously defined<br />
subarea. Can be called anytime during your scripts execution.</p>
<p>[overlaywin, overlaywinRect] = <a href="PsychImaging" class="uri">PsychImaging</a>(‘GetOverlayWindow’, win);<br />
- Will return the handle to the ‘overlaywin’dow associated with the<br />
given ’win’dow, if any. Will abort with an error message if the ’win’dow<br />
doesn’t have an associated overylay window.<br />
Currently, only the CRS Bits+ box in Mono++ mode and the <a href="VPixx" class="uri">VPixx</a> <a href="DataPixx" class="uri">DataPixx</a><br />
box in M16 mode does support overlays. Other output drivers don’t support<br />
such a feature. See “help <a href="BitsPlusPlus" class="uri">BitsPlusPlus</a>” for subfunction<br />
’GetOverlayWindow’ for more explanations of the purpose and properties of<br />
overlay windows. The explanations apply to the <a href="DPixx" class="uri">DPixx</a> device as well if it<br />
is opened in videomode ‘M16WithOverlay’.</p>
<h3 id="the-following-commands-are-only-for-specialists">The following commands are only for specialists:</h3>
<p>[imagingMode, needStereomode] = <a href="PsychImaging" class="uri">PsychImaging</a>(‘FinalizeConfiguration’);<br />
- Finish the configuration phase for this window. This will compute an<br />
optimal configuration for all stages of the pipeline, but won’t apply it<br />
yet. You’ll have to call <a href="Screen" class="uri">Screen</a>(‘OpenWindow’, windowPtr, ……,<br />
imagingMode, …); with the returned ‘imagingMode’ + any other options<br />
you’d like to have for your window. After that, you’ll have to call<br />
<a href="PsychImaging" class="uri">PsychImaging</a>(‘PostConfiguration’) to really apply and setup all your<br />
configuration settings. If you don’t have unusual needs, you can simplify<br />
these steps by simply calling <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’, ….);<br />
with the same parameters that you’d pass to <a href="Screen" class="uri">Screen</a>(‘OpenWindow’, ….);<br />
<a href="PsychImaging" class="uri">PsychImaging</a> will perform all necessary steps to upon return, you’ll have<br />
your window properly configured.</p>
<p><a href="PsychImaging" class="uri">PsychImaging</a>(‘PostConfiguration’, windowPtr [, clearcolor]);<br />
- To be called after opening the onscreen window ‘windowPtr’.<br />
Performs all the setup work to be done after the window was created.</p>
<div class="code_header" style="text-align:right;">
<p><span style="float:left;">Path  </span> <span class="counter">Retrieve <a href=
  "https://raw.github.com/Psychtoolbox-3/Psychtoolbox-3/beta/Psychtoolbox/PsychGLImageProcessing/PsychImaging.m">current version from GitHub</a> | View <a href=
  "https://github.com/Psychtoolbox-3/Psychtoolbox-3/commits/beta/Psychtoolbox/PsychGLImageProcessing/PsychImaging.m">changelog</a></span></p>
</div>
<div class="code">
<p><code>Psychtoolbox/PsychGLImageProcessing/PsychImaging.m</code></p>
</div>
