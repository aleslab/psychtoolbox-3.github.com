<h2 id="psychtoolbox-psychhardware-psychvrtoolbox">[[Psychtoolbox]] › [[PsychHardware]] › [[PsychVRToolbox]]</h2>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>() - High level driver for VR HMD devices.</p>
<p>This driver bundles all the common high level functionality<br />
of different Virtual Reality Head mounted displays into one<br />
function.</p>
<p>It dispatches generic calls into appropriate device specific<br />
drivers as needed.</p>
<h3 id="usage">Usage:</h3>
<p>oldverbosity = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘Verbosity’ [, newverbosity]);<br />
- Get/Set level of verbosity for driver status messages, warning messages,<br />
error messages etc. ‘newverbosity’ is the optional new verbosity level,<br />
‘oldverbosity’ is the currently set verbosity level - ie. before changing<br />
it. Valid settings are: 0 = Silent, 1 = Errors only, 2 = Warnings, 3 = Info,<br />
4 = Debug.</p>
<p>hmd = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘AutoSetupHMD’ [, basicTask][, basicRequirements][, basicQuality][, vendor][, deviceIndex]);<br />
- Automatically detect the first connected HMD, set it up with reasonable<br />
default parameters, and return a device handle ‘hmd’ to it. If the system<br />
does not support any <a href="HMDs" class="uri">HMDs</a>, not even emulated ones, just does nothing and<br />
returns an empty handle, ie., hmd = [], so caller can cope with that.</p>
<p>Optional parameters: ‘basicTask’ what kind of task should be implemented.<br />
The default is ‘Tracked3DVR’, which means to setup for stereoscopic 3D<br />
rendering, driven by head motion tracking, for a fully immersive experience<br />
in some kind of 3D virtual world. This is the default if omitted. The task<br />
‘Stereoscopic’ sets up for display of stereoscopic stimuli, but without<br />
head tracking. ‘Monoscopic’ sets up for display of monocular stimuli, ie.<br />
the HMD is just used as a special kind of standard display monitor.</p>
<p>‘basicRequirements’ defines basic requirements for the task. Currently<br />
defined are the following strings which can be combined into a single<br />
‘basicRequirements’ string:</p>
<p>‘LowPersistence’ = Try to keep exposure time of visual images on the retina<br />
low if possible, ie., try to approximate a pulse-type display instead of a<br />
hold-type display if possible. This has no effect on the Oculus Rift DK1.<br />
On the Rift DK2 it will enable low persistence scanning of the OLED display<br />
panel, to light up each pixel only a fraction of a video refresh cycle duration.<br />
On the Rift CV1, low persistence is always active, so this setting is redundant.</p>
<p>‘DebugDisplay’ = Show the output which is displayed on the HMD inside the<br />
Psychtoolbox onscreen window as well. This will have a negative impact on<br />
performance, latency and timing of the HMD visual presentation, so should only<br />
be used for debugging, as it may cause a seriously degraded VR experience.<br />
By default, no such debug output is produced and the Psychtoolbox onscreen<br />
window is not actually displayed on the desktop. This option is silently ignored<br />
on the old classic Oculus driver at the moment.</p>
<p>‘Float16Display’ = Request rendering, compositing and display in 16 bpc float<br />
format on some HMD’s and drivers. This will ask Psychtoolbox to render and<br />
post-process stimuli in 16 bpc linear floating point format, and allocate 16 bpc<br />
half-float textures as final renderbuffers to be sent to the VR compositor.<br />
If the VR compositor takes advantage of the high source image precision is at<br />
the discretion of the compositor and HMD. By default, if this request is omitted,<br />
processing and display in sRGB format is requested from Psychtoolbox and the compositor<br />
on some drivers, e.g., for the Oculus 1.11+ runtime and Rift CV1, ie., a roughly<br />
gamma 2.2 8 bpc format is used, which is optimized for the gamma response curve of<br />
at least the Oculus Rift CV1 display. On other <a href="HMDs" class="uri">HMDs</a> or drivers, this option may do<br />
nothing and get silently ignored.</p>
<p>‘PerEyeFOV’ = Request use of per eye individual and asymmetric fields of view, even<br />
when the ‘basicTask’ was selected to be ‘Monoscopic’ or ‘Stereoscopic’. This allows<br />
for wider field of view in these tasks, but requires the usercode to adapt to these<br />
different and asymmetric fields of view for each eye, e.g., by selecting proper 3D<br />
projection matrices for each eye. If a ‘basicTask’ of ‘3DVR’ for non-tracked 3D, or<br />
(the default) ‘Tracked3DVR’ for head tracking driven 3D is selected, then that implies<br />
per-eye individual and asymmetric fields of view, iow. ‘PerEyeFOV’ is implied. For pure<br />
‘basicTask’ of ‘Monoscopic’ or ‘Stereoscopic’ for <a href="Screen" class="uri">Screen</a>() 2D drawing, the system uses<br />
identical and symmetric fields of view for both eyes by default, so ‘PerEyeFOV’ would<br />
be needed to override this choice. <a href="COMPATIBILITy" class="uri">COMPATIBILITy</a> NOTE: Psychtoolbox-3 releases before<br />
June 2017 always used identical and symmetric fields of view for both eyes, which was<br />
a bug. However the error made was very small, due to the imaging properties of the<br />
Oculus Rift DK2, essentially imperceptible to the unknowing observer with the naked<br />
eye. Releases starting June 2017 now use separate fields of view in 3D rendering<br />
modes, and optionally for 2D mono/stereo modes with this ‘PerEyeFOV’ opt-in parameter,<br />
so stimulus display may change slightly for the same HMD hardware and user-code,<br />
compared to older Psychtoolbox-3 releases. This change was crucial to accomodate the<br />
rather different imaging properties of the Oculus Rift CV1 and possible other future<br />
HMD’s.</p>
<p>‘FastResponse’ = Try to switch images with minimal delay and fast<br />
pixel switching time. This will enable OLED panel overdrive processing<br />
on the Oculus Rift DK1 and DK2. OLED panel overdrive processing is a<br />
relatively expensive post processing step. On the Rift CV1, and generally<br />
with the new Oculus v1.11+ runtime, this is always active, so ‘FastResponse’<br />
is redundant on such panels and drivers.</p>
<p>‘TimingSupport’ = Support some hardware specific means of timestamping<br />
or latency measurements. On the Rift DK1 this does nothing. On the DK2<br />
it enables dynamic prediction and timing measurements with the Rifts internal<br />
latency tester. This does nothing anymore on Rift CV1.</p>
<p>‘TimeWarp’ = Enable per eye image 2D timewarping via prediction of eye<br />
poses at scanout time. This mostly only makes sense for head-tracked 3D<br />
rendering. Depending on ‘basicQuality’ a more cheap or more expensive<br />
procedure is used. On the v1.11 Oculus runtime and Rift CV1, ‘TimeWarp’<br />
is always active, so this option is redundant.</p>
<p>These basic requirements get translated into a device specific set of<br />
settings. The settings can also be specific to the selected ‘basicTask’,<br />
and if a quality vs. performance / system load tradeoff is unavoidable<br />
then the ‘basicQuality’ parameter may modulate the strategy.</p>
<p>‘basicQuality’ defines the basic tradeoff between quality and required<br />
computational power. A setting of 0 gives lowest quality, but with the<br />
lowest performance requirements. A setting of 1 gives maximum quality at<br />
maximum computational load. Values between 0 and 1 change the quality to<br />
performance tradeoff.</p>
<p>By default all currently supported types of <a href="HMDs" class="uri">HMDs</a> from different<br />
vendors are probed and the first one found is used. If the optional<br />
parameter ‘vendor’ is provided, only devices from that vendor are<br />
detected and the first detected device is chosen.</p>
<p>If additionally the optional ‘deviceIndex’ parameter is provided then<br />
that specific device ‘deviceIndex’ from that ‘vendor’ is opened and set up.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetAutoClose’, hmd, mode);<br />
- Set autoclose mode for HMD with handle ‘hmd’. ‘mode’ can be<br />
0 (this is the default) to not do anything special. 1 will close<br />
the HMD ‘hmd’ when the onscreen window is closed which displays<br />
on the HMD. 2 will do the same as 1, but close all open <a href="HMDs" class="uri">HMDs</a> and<br />
shutdown the complete driver and runtime - a full cleanup.</p>
<p>isOpen = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘IsOpen’, hmd);<br />
- Returns 1 if ‘hmd’ corresponds to an open HMD, 0 otherwise.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘<a href="Close" class="uri">Close</a>’ [, hmd])<br />
- <a href="Close" class="uri">Close</a> provided HMD device ‘hmd’. If no ‘hmd’ handle is provided,<br />
all <a href="HMDs" class="uri">HMDs</a> will be closed and the driver will be shutdown.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘Controllers’, hmd);<br />
- Return a bitmask of all connected controllers: Can be the bitand<br />
of the OVR.<a href="ControllerType" class="uri">ControllerType</a>_XXX flags described in ‘GetInputState’.<br />
This does not detect if controllers are hot-plugged or unplugged after<br />
the HMD was opened. Iow. only probed at ‘Open’.</p>
<p>info = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetInfo’, hmd);<br />
- Retrieve a struct ‘info’ with information about the HMD ‘hmd’.<br />
The returned info struct contains at least the following standardized<br />
fields with information:<br />
handle = Driver internal handle for the specific HMD.<br />
driver = Function handle to the actual driver for the HMD, e.g., @<a href="PsychOculusVR" class="uri">PsychOculusVR</a>.<br />
type = Defines the type/vendor of the device, e.g., ‘Oculus’.<br />
subtype = Defines the type of driver more specific, e.g., ‘Oculus-classic’ or ‘Oculus-1’.<br />
modelName = Name string with the name of the model of the device, e.g., ‘Rift DK2’.</p>
<p>separateEyePosesSupported = 1 if use of <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetEyePose’) will improve<br />
the quality of the VR experience, 0 if no improvement<br />
is to be expected, so ‘GetEyePose’ can be avoided<br />
to save processing time without a loss of quality.</p>
<p><a href="VRControllersSupported" class="uri">VRControllersSupported</a> = 1 if use of <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetInputState’) will provide input<br />
from actual dedicated VR controllers. Value is 0 if<br />
controllers are only emulated to some limited degree,<br />
e.g., by abusing a regular keyboard as a button controller,<br />
ie. mapping keyboard keys to OVR.Button_XXX buttons.</p>
<p>handTrackingSupported = 1 if <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘PrepareRender’) with reqmask +2 will provide<br />
valid hand tracking info, 0 if this is not supported and will<br />
just report fake values. A driver may report 1 here but still<br />
don’t provide meaningful info at runtime, e.g., if required<br />
tracking hardware is missing or gets disconnected. The flag<br />
just aids extra performance optimizations in your code.</p>
<p>hapticFeedbackSupported = 1 if basic haptic feedback is supported in principle on some controllers.<br />
0 otherwise. A flag of zero means no haptic feedback support, but<br />
a flag of 1 may still mean no actual feedback, e.g., if suitable<br />
hardware is not configured and present. Flags higher than 1 can<br />
signal presence of more advanced haptic feedback, so you should<br />
test for a setting == 1 to know if <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘HapticPulse’) works<br />
in principle, which is considered basic feedback ability.</p>
<p>The info struct may contain much more vendor specific information, but the above<br />
set is supported across all devices.</p>
<p>[isVisible, playAreaBounds, <a href="OuterAreaBounds" class="uri">OuterAreaBounds</a>] = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘VRAreaBoundary’, hmd [, requestVisible]);<br />
- Request visualization of the VR play area boundary for ‘hmd’ and returns its<br />
current extents.</p>
<p>‘requestVisible’ 1 = Request showing the boundary area markers, 0 = Don’t<br />
request showing the markers.<br />
If the driver can control area boundary visibility is highly dependent on the VR<br />
driver in use. This flag may get ignored. See driver specific help, e.g.,<br />
“help <a href="PsychOculusVR1" class="uri">PsychOculusVR1</a>”, for behaviour of a specific driver.</p>
<p>Some drivers or hardware setups may not supports VR area boundaries at all, in<br />
which case the function will return empty boundaries.</p>
<p>Returns in ‘isVisible’ the current visibility status of the VR area boundaries.</p>
<p>‘playAreaBounds’ is a 3-by-n matrix defining the play area boundaries. Each<br />
column represents the [x;y;z] coordinates of one 3D definition point. Connecting<br />
successive points by line segments defines the boundary, as projected onto the<br />
floor. Points are listed in clock-wise direction. An empty return argument means<br />
that the play area is so far undefined.</p>
<p>‘OuterAreaBounds’ defines the outer area boundaries in the same way as<br />
‘playAreaBounds’.</p>
<p>input = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetInputState’, hmd, controllerType);<br />
- Get input state of controller ‘controllerType’ associated with HMD ‘hmd’.</p>
<p>Note that if the underlying driver does not support special VR controllers, ie.,<br />
hmdinfo = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetInfo’) returns hmdinfo.<a href="VRControllersSupported" class="uri">VRControllersSupported</a> == 0, then<br />
only a minimally useful ‘input’ state is returned, which is based on emulating or<br />
faking input from real controllers, so this function will be of limited use. Specifically,<br />
on emulated controllers, only the input.Time and input.Buttons fields are returned, all<br />
other fields are missing.</p>
<p>‘controllerType’ can be one of OVR.<a href="ControllerType" class="uri">ControllerType</a>_LTouch, OVR.<a href="ControllerType" class="uri">ControllerType</a>_RTouch,<br />
OVR.<a href="ControllerType" class="uri">ControllerType</a>_Touch, OVR.<a href="ControllerType" class="uri">ControllerType</a>_Remote, OVR.<a href="ControllerType" class="uri">ControllerType</a>_XBox, or<br />
OVR.<a href="ControllerType" class="uri">ControllerType</a>_Active for selecting whatever controller is currently active.</p>
<p>Return argument ‘input’ is a struct with fields describing the state of buttons and<br />
other input elements of the specified ‘controllerType’. It has the following fields:</p>
<p>‘Time’ Time of last input state change of controller.<br />
‘Buttons’ Vector with button state on the controller, similar to the ‘keyCode’<br />
vector returned by <a href="KbCheck" class="uri">KbCheck</a>() for regular keyboards. Each position in the vector<br />
reports pressed (1) or released (0) state of a specific button. Use the OVR.Button_XXX<br />
constants to map buttons to positions.</p>
<p>‘Touches’ Like ‘Buttons’ but for touch buttons. Use the OVR.Touch_XXX constants to map<br />
touch points to positions.</p>
<p>‘Trigger’(1/2) = Left (1) and Right (2) trigger: Value range 0.0 - 1.0, filtered and with dead-zone.<br />
‘TriggerNoDeadzone’(1/2) = Left (1) and Right (2) trigger: Value range 0.0 - 1.0, filtered.<br />
‘TriggerRaw’(1/2) = Left (1) and Right (2) trigger: Value range 0.0 - 1.0, raw values unfiltered.<br />
‘Grip’(1/2) = Left (1) and Right (2) grip button: Value range 0.0 - 1.0, filtered and with dead-zone.<br />
‘GripNoDeadzone’(1/2) = Left (1) and Right (2) grip button: Value range 0.0 - 1.0, filtered.<br />
‘GripRaw’(1/2) = Left (1) and Right (2) grip button: Value range 0.0 - 1.0, raw values unfiltered.</p>
<p>‘Thumbstick’ = 2x2 matrix: Column 1 contains left thumbsticks [x;y] axis values, column 2 contains<br />
right sticks [x;y] axis values. Values are in range -1 to +1, filtered and with deadzone applied.<br />
‘ThumbstickNoDeadzone’ = Like ‘Thumbstick’, filtered, but without a deadzone applied.<br />
‘ThumbstickRaw’ = ‘Thumbstick’ raw date without deadzone or filtering applied.</p>
<p>pulseEndTime = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘HapticPulse’, hmd, controllerType [, duration=XX][, freq=1.0][, amplitude=1.0]);<br />
- Trigger a haptic feedback pulse, some controller vibration, on the specified ‘controllerType’<br />
associated with the specified ‘hmd’. ‘duration’ is desired pulse duration in seconds. On Oculus<br />
devices, by default a maximum of 2.5 seconds pulse is executed, but other vendors devices may have<br />
a different maximum. ‘freq’ is normalized frequency in range 0.0 - 1.0. A value of 0 will try to<br />
disable an ongoing pulse. How this normalized ‘freq’ maps to a specific haptic device is highly<br />
device and runtime dependent.</p>
<p>‘amplitude’ is the amplitude of the vibration in normalized 0.0 - 1.0 range.</p>
<p>‘pulseEndTime’ returns the expected stop time of vibration in seconds, given the parameters.<br />
Currently the function will return immediately for a (default) ‘duration’, and the pulse<br />
will end after the maximum duration supported by the given device. Smaller ‘duration’ values than<br />
the maximum duration will block the execution of the function until the ‘duration’ has passed on<br />
some types of controllers.</p>
<p>Please note that behaviour of this function is highly dependent on the type of VR driver and<br />
devices used. You should consult driver specific documentation for details, e.g., the help of<br />
‘PsychOculusVR’ or ‘PsychOculusVR1’ for Oculus systems. On some drivers the function may do<br />
nothing at all, e.g., if the ‘GetInfo’ function returns info.hapticFeedbackSupported == 0.</p>
<p>state = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘PrepareRender’, hmd [, userTransformMatrix][, reqmask=1][, targetTime]);<br />
- Mark the start of the rendering cycle for a new 3D rendered stereoframe.<br />
Return a struct ‘state’ which contains various useful bits of information<br />
for 3D stereoscopic rendering of a scene, based on head tracking data.</p>
<p>‘hmd’ is the handle of the HMD which delivers tracking data and receives the<br />
rendered content for display.</p>
<p>‘reqmask’ defines what kind of information is requested to be returned in<br />
struct ‘state’. Only query information you actually need, as computing some<br />
of this info is expensive! See below for supported values for ‘reqmask’.</p>
<p>‘targetTime’ is the expected time at which the rendered frame will display.<br />
This could potentially be used by the driver to make better predictions of<br />
camera/eye/head pose for the image. Omitting the value will use a target time<br />
that is implementation specific, but known to give generally good results,<br />
e.g., the midpoint of scanout of the next video frame.</p>
<p>‘userTransformMatrix’ is an optional 4x4 right hand side (RHS) transformation<br />
matrix. It gets applied to the tracked head pose as a global transformation<br />
before computing results based on head pose like, e.g., camera transformations.<br />
You can use this to translate the “virtual head” and thereby the virtual eyes/<br />
cameras in the 3D scene, so observer motion is not restricted to the real world<br />
tracking volume of your headset. A typical ‘userTransformMatrix’ would be a<br />
combined translation and rotation matrix to position the observer at some<br />
3D location in space, then define his/her global looking direction, aka as<br />
heading angle, yaw orientation, or rotation around the y-axis in 3D space.<br />
Head pose tracking results would then operate relative to this global transform.<br />
If ‘userTransformMatrix’ is left out, it will default to an identity transform,<br />
in other words, it will do nothing.</p>
<p>state always contains a field state.tracked, whose bits signal the status<br />
of head tracking for this frame. A +1 flag means that head orientation is<br />
tracked. A +2 flag means that head position is tracked via some absolute<br />
position tracker like, e.g., the Oculus Rift DK2 or Rift CV1 camera. A +128<br />
flag means the HMD is actually strapped onto the subjects head and displaying<br />
our visual content. Lack of this flag means the HMD is off and thereby blanked<br />
and dark, or we lost access to it to another application.</p>
<p>state also always contains a field state.<a href="SessionState" class="uri">SessionState</a>, whose bits signal general<br />
VR session status:</p>
<p>+1 = Our rendering goes to the HMD, ie. we have control over it. Lack of this could<br />
mean the Health and Safety warning is displaying at the moment and waiting for<br />
acknowledgement, or the Oculus GUI application is in control.<br />
+2 = HMD is present and active.<br />
+4 = HMD is strapped onto users head. E.g., a Oculus Rift CV1 would switch off/blank<br />
if not on the head.<br />
+8 = <a href="DisplayLost" class="uri">DisplayLost</a> condition! Some hardware/software malfunction, need to completely quit this<br />
Psychtoolbox session to recover from this.<br />
+16 = <a href="ShouldQuit" class="uri">ShouldQuit</a> The user interface / user asks us to voluntarily terminate this session.<br />
+32 = <a href="ShouldRecenter" class="uri">ShouldRecenter</a> = The user interface asks us to recenter/recalibrate our tracking origin.</p>
<h3 id="reqmask-defaults-to-1-and-can-have-the-following-values-added-together">‘reqmask’ defaults to 1 and can have the following values added together:</h3>
<p>+1 = Return matrices for left and right “eye cameras” which can be directly<br />
used as <a href="OpenGL" class="uri">OpenGL</a> GL_MODELVIEW matrices for rendering the scene. 4x4 matrices<br />
for left- and right eye are contained in state.modelView{1} and {2}.</p>
<pre><code> Return position and orientation 4x4 camera view matrices which describe  
 position and orientation of the &quot;eye cameras&quot; relative to the world  
 reference frame. They are the inverses of state.modelView{}. These  
 matrices can be directly used to define cameras for rendering of complex  
 3D scenes with the [Horde3D](Horde3D) 3D engine. Left- and right eye matrices are  
 contained in state.cameraView{1} and {2}.  

 Additionally tracked/predicted head pose is returned in state.localHeadPoseMatrix  
 and the global head pose after application of the &#39;userTransformMatrix&#39; is  
 returned in state.globalHeadPoseMatrix - this is the basis for computing  
 the camera transformation matrices.  </code></pre>
<p>+2 = Return matrices for tracked left and right hands of user, ie. of tracked positions<br />
and orientations of left and right hand tracking controllers, if any. See also<br />
section about ‘GetInfo’ for some performance comments.</p>
<pre><code> state.handStatus(1) = Tracking status of left hand: 0 = Untracked, 1 = Orientation  
                       tracked, 2 = Position tracked, 3 = Orientation and position  
                       tracked. If handStatus is == 0 then all the following information  
                       is invalid and can not be used in any meaningful way.  

 state.handStatus(2) = Tracking status of right hand.  

 state.localHandPoseMatrix{1} = 4x4 [OpenGL](OpenGL) right handed reference frame matrix with  
                                hand position and orientation encoded to define a  
                                proper GL\_MODELVIEW transform for rendering stuff  
                                &quot;into&quot;/&quot;relative to&quot; the oriented left hand.  

 state.localHandPoseMatrix{2} = Ditto for the right hand.  

 state.globalHandPoseMatrix{1} = userTransformMatrix \* state.localHandPoseMatrix{1};  
                                 Left hand pose transformed by passed in userTransformMatrix.  

 state.globalHandPoseMatrix{2} = Ditto for the right hand.  

 state.globalHandPoseInverseMatrix{1} = Inverse of globalHandPoseMatrix{1} for collision  
                                        testing/grasping of virtual objects relative to  
                                        hand pose of left hand.  

 state.globalHandPoseInverseMatrix{2} = Ditto for right hand.  </code></pre>
<p>More flags to follow…</p>
<p>eyePose = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetEyePose’, hmd, renderPass [, userTransformMatrix][, targetTime]);<br />
- Return a struct ‘eyePose’ which contains various useful bits of information<br />
for 3D stereoscopic rendering of the stereo view of one eye, based on head<br />
tracking data. This function provides essentially the same information as<br />
the ‘PrepareRender’ function, but only for one eye. Therefore you will need<br />
to call this function twice, once for each of the two renderpasses, at the<br />
beginning of each renderpass.</p>
<p>Note: The ‘GetEyePose’ function may not be implemented in a meaningful/beneficial<br />
way for all supported types of HMD. This means that while the function will work<br />
on all supported <a href="HMDs" class="uri">HMDs</a>, there may not be any benefit of using it in terms of<br />
performance or quality of the VR experience, because the underlying driver may<br />
only emulate / fake the results for compatibility. Currently only the driver for<br />
the Oculus VR Rift DK1 and DK2 supports this function in a way that will improve<br />
the VR experience, the status for future Oculus <a href="HMDs" class="uri">HMDs</a>, or <a href="HMDs" class="uri">HMDs</a> from other vendors<br />
is currently unknown. The info struct returned by <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetInfo’) will return<br />
info.separateEyePosesSupported == 1 if there is a benefit to be expected from use<br />
of this function, or info.separateEyePosesSupported == 0 if no benefit is expected<br />
and simply using the data from <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘PrepareRender’) will provide results with<br />
the same quality at a lower computational cost.</p>
<p>‘hmd’ is the handle of the HMD which delivers tracking data and receives the<br />
rendered content for display.</p>
<p>‘renderPass’ defines if information should be returned for the 1st renderpass<br />
(renderPass == 0) or for the 2nd renderpass (renderPass == 1). The driver will<br />
decide for you if the 1st renderpass should render the left eye and the 2nd<br />
pass the right eye, or if the 1st renderpass should render the right eye and<br />
then the 2nd renderpass the left eye. The ordering depends on the properties<br />
of the video display of your HMD, specifically on the video scanout order:<br />
Is it right to left, left to right, or top to bottom? For each scanout order<br />
there is an optimal order for the renderpasses to minimize perceived lag.</p>
<p>‘targetTime’ is the expected time at which the rendered frame will display.<br />
This could potentially be used by the driver to make better predictions of<br />
camera/eye/head pose for the image. Omitting the value will use a target time<br />
that is implementation specific, but known to give generally good results.</p>
<p>‘userTransformMatrix’ is an optional 4x4 right hand side (RHS) transformation<br />
matrix. It gets applied to the tracked head pose as a global transformation<br />
before computing results based on head pose like, e.g., camera transformations.<br />
You can use this to translate the “virtual head” and thereby the virtual eyes/<br />
cameras in the 3D scene, so observer motion is not restricted to the real world<br />
tracking volume of your headset. A typical ‘userTransformMatrix’ would be a<br />
combined translation and rotation matrix to position the observer at some<br />
3D location in space, then define his/her global looking direction, aka as<br />
heading angle, yaw orientation, or rotation around the y-axis in 3D space.<br />
Head pose tracking results would then operate relative to this global transform.<br />
If ‘userTransformMatrix’ is left out, it will default to an identity transform,<br />
in other words, it will do nothing.</p>
<h3 id="return-values-in-struct-eyepose">Return values in struct ‘eyePose’:</h3>
<p>‘eyeIndex’ The eye for which this information applies. 0 = Left eye, 1 = Right eye.<br />
You can pass ‘eyeIndex’ into the <a href="Screen" class="uri">Screen</a>(‘SelectStereoDrawBuffer’, win, eyeIndex)<br />
to select the proper eye target render buffer.</p>
<p>‘modelView’ is a 4x4 RHS <a href="OpenGL" class="uri">OpenGL</a> matrix which can be directly used as <a href="OpenGL" class="uri">OpenGL</a><br />
GL_MODELVIEW matrix for rendering the scene.</p>
<p>‘cameraView’ contains a 4x4 RHS camera matrix which describes position and<br />
orientation of the “eye camera” relative to the world reference<br />
frame. It is the inverse of eyePose.modelView. This matrix can<br />
be directly used to define the camera for rendering of complex<br />
3D scenes with the <a href="Horde3D" class="uri">Horde3D</a> 3D engine or other engines which want<br />
absolute camera pose instead of the inverse matrix.</p>
<p>Additionally tracked/predicted head pose is returned in eyePose.localHeadPoseMatrix<br />
and the global head pose after application of the ‘userTransformMatrix’ is<br />
returned in eyePose.globalHeadPoseMatrix - this is the basis for computing<br />
the camera transformation matrix.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetupRenderingParameters’, hmd [, basicTask=‘Tracked3DVR’][, basicRequirements][, basicQuality=0][, fov=[<a href="HMDRecommended" class="uri">HMDRecommended</a>]][, pixelsPerDisplay=1])<br />
- Query the HMD ‘hmd’ for its properties and setup internal rendering<br />
parameters in preparation for opening an onscreen window with <a href="PsychImaging" class="uri">PsychImaging</a><br />
to display properly on the HMD. See section about ‘AutoSetupHMD’ above for<br />
the meaning of the optional parameters ‘basicTask’, ‘basicRequirements’<br />
and ‘basicQuality’.</p>
<p>‘fov’ Optional field of view in degrees, from line of sight: [leftdeg, rightdeg,<br />
updeg, downdeg]. If ‘fov’ is omitted, the HMD runtime will be asked for a<br />
good default field of view and that will be used. The field of view may be<br />
dependent on the settings in the HMD user profile of the currently selected<br />
user.</p>
<p>‘pixelsPerDisplay’ Ratio of the number of render target pixels to display pixels<br />
at the center of distortion. Defaults to 1.0 if omitted. Lower values can<br />
improve performance, at lower quality.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetBasicQuality’, hmd, basicQuality);<br />
- Set basic level of quality vs. required GPU performance.</p>
<p>oldSetting = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetFastResponse’, hmd [, enable]);<br />
- Return old setting for ‘FastResponse’ mode in ‘oldSetting’,<br />
optionally disable or enable the mode via specifying the ‘enable’<br />
parameter as 0 or &gt; 0. Please note that if you want to use ‘FastResponse’,<br />
you must request and thereby enable it at the beginning of a session, as<br />
the driver must do some neccessary setup prep work at startup of the HMD.<br />
Once it was initially enabled, you can switch the setting at runtime with<br />
this function.</p>
<p>Some drivers may implement different strategies for ‘FastResponse’, selectable<br />
via different settings for the ‘enable’ flag here. E.g., the Oculus Rift driver<br />
support three different methods 1, 2 and 3 at the moment.</p>
<p>oldSetting = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetTimeWarp’, hmd [, enable]);<br />
- Return old setting for ‘TimeWarp’ mode in ‘oldSetting’,<br />
optionally enable or disable the mode via specifying the ‘enable’<br />
parameter as 1 or 0. Please note that if you want to use ‘TimeWarp’,<br />
you must request and thereby enable it at the beginning of a session, as<br />
the driver must do some neccessary setup prep work at startup of the HMD.<br />
Once it was initially enabled, you can switch the setting at runtime with<br />
this function.</p>
<p>oldSetting = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetLowPersistence’, hmd [, enable]);<br />
- Return old setting for ‘LowPersistence’ mode in ‘oldSetting’,<br />
optionally enable or disable the mode via specifying the ‘enable’<br />
parameter as 1 or 0.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetHSWDisplayDismiss’, hmd [, dismissTypes=1+2+4]);<br />
- Set how the user can dismiss the “Health and safety warning display”.<br />
‘dismissTypes’ can be -1 to disable the HSWD, or a value &gt;= 0 to show<br />
the HSWD until a timeout and or until the user dismisses the HSWD.<br />
The following flags can be added to define type of dismissal:</p>
<p>+0 = Display until timeout, if any. Will wait forever if there isn’t any timeout!<br />
+1 = Dismiss via keyboard keypress.<br />
+2 = Dismiss via mouse click or mousepad tap.<br />
+4 = Dismiss via a tap to the HMD (detected via accelerometer).</p>
<p>[bufferSize, imagingFlags, stereoMode] = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetClientRenderingParameters’, hmd);<br />
- Retrieve recommended size in pixels ‘bufferSize’ = [width, height] of the client<br />
renderbuffer for each eye for rendering to the HMD. Returns parameters<br />
previously computed by <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘SetupRenderingParameters’, hmd).</p>
<p>Also returns ‘imagingFlags’, the required imaging mode flags for setup of<br />
the <a href="Screen" class="uri">Screen</a> imaging pipeline. Also returns the needed ‘stereoMode’ for the<br />
pipeline.</p>
<p>This function is usually called by <a href="PsychImaging" class="uri">PsychImaging</a>(), you don’t need to deal<br />
with it.</p>
<p>needPanelFitter = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetPanelFitterParameters’, hmd);<br />
- ‘needPanelFitter’ is 1 if a custom panel fitter tasks is needed, and ‘bufferSize’<br />
from the <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetClientRenderingParameters’, hmd); defines the size of the<br />
clientRect for the onscreen window. ‘needPanelFitter’ is 0 if no panel fitter is<br />
needed.</p>
<p>[winRect, ovrfbOverrideRect, ovrSpecialFlags] = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘OpenWindowSetup’, hmd, screenid, winRect, ovrfbOverrideRect, ovrSpecialFlags);<br />
- Compute special override parameters for given input/output arguments, as needed<br />
for a specific HMD. Take other preparatory steps as needed, immediately before the<br />
<a href="Screen" class="uri">Screen</a>(‘OpenWindow’) command executes. This is called as part of <a href="PsychImaging" class="uri">PsychImaging</a>(‘OpenWindow’),<br />
with the user provided hmd, screenid, winRect etc.</p>
<p>isOutput = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘IsHMDOutput’, hmd, scanout);<br />
- Returns 1 (true) if ‘scanout’ describes the video output to which the<br />
HMD ‘hmd’ is connected. ‘scanout’ is a struct returned by the <a href="Screen" class="uri">Screen</a><br />
function <a href="Screen" class="uri">Screen</a>(‘ConfigureDisplay’, ‘Scanout’, screenid, outputid);<br />
This allows probing video outputs to find the one which feeds the HMD.</p>
<p>[headToEyeShiftv, headToEyeShiftMatrix] = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetEyeShiftVector’, hmd, eye);<br />
- Retrieve 3D translation vector [tx, ty, tz] that defines the 3D position of the given<br />
eye ‘eye’ for the given HMD ‘hmd’, relative to the origin of the local head/HMD<br />
reference frame. This is needed to translate a global head pose into a eye<br />
pose, e.g., to translate the output of <a href="PsychOculusVR" class="uri">PsychOculusVR</a>(‘GetEyePose’) into actual<br />
tracked/predicted eye locations for stereo rendering.</p>
<p>In addition to the ‘headToEyeShiftv’ vector, a corresponding 4x4 translation<br />
matrix is also returned in ‘headToEyeShiftMatrix’ for convenience.</p>
<p>[projL, projR] = <a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘GetStaticRenderParameters’, hmd [, clipNear=0.01][, clipFar=10000.0]);<br />
- Retrieve parameters needed to setup the intrinsic parameters of the virtual<br />
camera for scene rendering.</p>
<p>‘clipNear’ Optional near clipping plane for <a href="OpenGL" class="uri">OpenGL</a>. Defaults to 0.01.<br />
‘clipFar’ Optional far clipping plane for <a href="OpenGL" class="uri">OpenGL</a>. Defaults to 10000.0.</p>
<h3 id="return-arguments">Return arguments:</h3>
<p>‘projL’ is the 4x4 <a href="OpenGL" class="uri">OpenGL</a> projection matrix for the left eye rendering.<br />
‘projR’ is the 4x4 <a href="OpenGL" class="uri">OpenGL</a> projection matrix for the right eye rendering.<br />
Please note that projL and projR are usually identical for typical rendering<br />
scenarios.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘Start’, hmd);<br />
- Start live operations of the ‘hmd’, e.g., head tracking.</p>
<p><a href="PsychVRHMD" class="uri">PsychVRHMD</a>(‘Stop’, hmd);<br />
- Stop live operations of the ‘hmd’, e.g., head tracking.</p>
<div class="code_header" style="text-align:right;">
<p><span style="float:left;">Path  </span> <span class="counter">Retrieve <a href=
  "https://raw.github.com/Psychtoolbox-3/Psychtoolbox-3/beta/Psychtoolbox/PsychHardware/PsychVRToolbox/PsychVRHMD.m">current version from GitHub</a> | View <a href=
  "https://github.com/Psychtoolbox-3/Psychtoolbox-3/commits/beta/Psychtoolbox/PsychHardware/PsychVRToolbox/PsychVRHMD.m">changelog</a></span></p>
</div>
<div class="code">
<p><code>Psychtoolbox/PsychHardware/PsychVRToolbox/PsychVRHMD.m</code></p>
</div>
